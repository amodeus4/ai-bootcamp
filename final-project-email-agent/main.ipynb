{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c72654b8-5430-4f76-8a7f-1ac72e3ee330",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m240 packages\u001b[0m \u001b[2min 5.92s\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`transformers==4.57.0` is yanked (reason: \"Error in the setup causing installation issues\")\u001b[0m\n",
      "\u001b[2K\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`transformers==4.57.0` is yanked (reason: \"Error in the setup causing installation issues\")\u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m10 packages\u001b[0m \u001b[2min 353ms\u001b[0m\u001b[0m                                            \n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/10] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m10 packages\u001b[0m \u001b[2min 155ms\u001b[0m\u001b[0ment==2.185.0                  \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-api-core\u001b[0m\u001b[2m==2.28.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-api-python-client\u001b[0m\u001b[2m==2.185.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-auth-httplib2\u001b[0m\u001b[2m==0.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-auth-oauthlib\u001b[0m\u001b[2m==1.2.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttplib2\u001b[0m\u001b[2m==0.31.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1moauthlib\u001b[0m\u001b[2m==3.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mproto-plus\u001b[0m\u001b[2m==1.26.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyparsing\u001b[0m\u001b[2m==3.2.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests-oauthlib\u001b[0m\u001b[2m==2.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1muritemplate\u001b[0m\u001b[2m==4.2.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!uv add google-auth-oauthlib google-auth-httplib2 google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba897032-16aa-4637-9b78-1e872568ea43",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m245 packages\u001b[0m \u001b[2min 251ms\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`transformers==4.57.0` is yanked (reason: \"Error in the setup causing installation issues\")\u001b[0m\n",
      "\u001b[2K\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`transformers==4.57.0` is yanked (reason: \"Error in the setup causing installation issues\")\u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m5 packages\u001b[0m \u001b[2min 88ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/5] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m5 packages\u001b[0m \u001b[2min 52ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1met-xmlfile\u001b[0m\u001b[2m==2.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlxml\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenpyxl\u001b[0m\u001b[2m==3.1.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpypdf2\u001b[0m\u001b[2m==3.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-docx\u001b[0m\u001b[2m==1.2.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!uv add PyPDF2 python-docx openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e582a2-62cd-4586-b88f-9be841f8f9c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m245 packages\u001b[0m \u001b[2min 110ms\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`transformers==4.57.0` is yanked (reason: \"Error in the setup causing installation issues\")\u001b[0m\n",
      "\u001b[2K\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`transformers==4.57.0` is yanked (reason: \"Error in the setup causing installation issues\")\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m237 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!uv add pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe25dab-7090-487a-9627-7ac30f4d4288",
   "metadata": {},
   "source": [
    "# Gmail Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8105865-9861-49fd-b44c-b90e58c4ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/gmail.readonly',\n",
    "          'https://www.googleapis.com/auth/gmail.modify']\n",
    "\n",
    "def authenticate_gmail():\n",
    "    \"\"\"Authenticate and return Gmail API service\"\"\"\n",
    "    creds = None\n",
    "    \n",
    "   \n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    \n",
    "    # If no valid credentials, let user log in\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        \n",
    "        # save credentials for next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "    \n",
    "    return build('gmail', 'v1', credentials=creds)\n",
    "\n",
    "\n",
    "service = authenticate_gmail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9864220-3eda-48b0-9e76-4f96ee0678df",
   "metadata": {},
   "source": [
    "# Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "434ecc01-b9e4-476b-be46-438cacf32cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes loaded!\n"
     ]
    }
   ],
   "source": [
    "# structuring data before storing it in Elasticsearch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "import base64\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EmailAttachment:\n",
    "    \"\"\"Represents an email attachment\"\"\"\n",
    "    filename: str\n",
    "    mime_type: str\n",
    "    attachment_id: str\n",
    "    size: int\n",
    "    content: Optional[bytes] = None\n",
    "    \n",
    "    def to_es_dict(self) -> Dict:\n",
    "        \"\"\"Convert to Elasticsearch-friendly format\"\"\"\n",
    "        return {\n",
    "            'filename': self.filename,\n",
    "            'mime_type': self.mime_type,\n",
    "            'size': self.size,\n",
    "            'has_content': self.content is not None\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EmailDocument:\n",
    "    \"\"\"Elasticsearch-optimized email document structure\"\"\"\n",
    "    \n",
    "    # Required fields first \n",
    "    email_id: str\n",
    "    thread_id: str\n",
    "    sender_name: str\n",
    "    sender_email: str\n",
    "    subject: str\n",
    "    \n",
    "    # Optional fields with defaults\n",
    "    recipients: List[str] = field(default_factory=list)\n",
    "    cc: List[str] = field(default_factory=list)\n",
    "    bcc: List[str] = field(default_factory=list)\n",
    "    body_plain: str = \"\"\n",
    "    body_html: str = \"\"\n",
    "    snippet: str = \"\"\n",
    "    date: datetime = field(default_factory=datetime.now)\n",
    "    processed_at: Optional[datetime] = None\n",
    "    attachments: List[EmailAttachment] = field(default_factory=list)\n",
    "    labels: List[str] = field(default_factory=list)\n",
    "    category: Optional[str] = None\n",
    "    priority: str = \"normal\"\n",
    "    is_read: bool = False\n",
    "    is_important: bool = False\n",
    "    is_starred: bool = False\n",
    "    has_attachments: bool = False\n",
    "    extracted_data: Dict[str, Any] = field(default_factory=dict)\n",
    "    embedding: Optional[List[float]] = None\n",
    "    action_items: List[Dict[str, Any]] = field(default_factory=list)\n",
    "    processing_status: str = \"pending\"\n",
    "    processing_errors: List[str] = field(default_factory=list)\n",
    "    \n",
    "    def to_es_document(self) -> Dict:\n",
    "        \"\"\"Convert to Elasticsearch document format\"\"\"\n",
    "        return {\n",
    "            '_id': self.email_id,\n",
    "            '_source': {\n",
    "                'email_id': self.email_id,\n",
    "                'thread_id': self.thread_id,\n",
    "                'sender': {\n",
    "                    'name': self.sender_name,\n",
    "                    'email': self.sender_email\n",
    "                },\n",
    "                'recipients': self.recipients,\n",
    "                'cc': self.cc,\n",
    "                'bcc': self.bcc,\n",
    "                'subject': self.subject,\n",
    "                'body': {\n",
    "                    'plain': self.body_plain,\n",
    "                    'html': self.body_html\n",
    "                },\n",
    "                'snippet': self.snippet,\n",
    "                'date': self.date.isoformat(),\n",
    "                'processed_at': self.processed_at.isoformat() if self.processed_at else None,\n",
    "                'attachments': [att.to_es_dict() for att in self.attachments],\n",
    "                'has_attachments': len(self.attachments) > 0,\n",
    "                'attachment_count': len(self.attachments),\n",
    "                'labels': self.labels,\n",
    "                'category': self.category,\n",
    "                'priority': self.priority,\n",
    "                'is_read': self.is_read,\n",
    "                'is_important': self.is_important,\n",
    "                'is_starred': self.is_starred,\n",
    "                'extracted_data': self.extracted_data,\n",
    "                'action_items': self.action_items,\n",
    "                'action_count': len(self.action_items),\n",
    "                'processing_status': self.processing_status,\n",
    "                'processing_errors': self.processing_errors,\n",
    "                'embedding': self.embedding\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_context_for_llm(self, include_body: bool = True, max_body_length: int = 1000) -> str:\n",
    "        \"\"\"Get formatted context for LLM\"\"\"\n",
    "        body = self.body_plain[:max_body_length] if include_body else self.snippet\n",
    "        if len(self.body_plain) > max_body_length and include_body:\n",
    "            body += \"\\n[... truncated ...]\"\n",
    "        \n",
    "        return f\"\"\"\n",
    "Subject: {self.subject}\n",
    "From: {self.sender_name} <{self.sender_email}>\n",
    "Date: {self.date.strftime('%Y-%m-%d %H:%M')}\n",
    "Body: {body}\n",
    "Attachments: {len(self.attachments)} file(s)\n",
    "        \"\"\".strip()\n",
    "\n",
    "\n",
    "class ElasticsearchEmailStore:\n",
    "    \"\"\"Manages email documents in Elasticsearch\"\"\"\n",
    "    \n",
    "    def __init__(self, es_host: str = \"localhost\", es_port: int = 9200, \n",
    "                 index_name: str = \"emails\"):\n",
    "        self.es = Elasticsearch([f\"http://{es_host}:{es_port}\"])\n",
    "        self.index_name = index_name\n",
    "        \n",
    "        # Query ES to check if index exists\n",
    "        if not self.es.indices.exists(index=self.index_name):\n",
    "            print(f\"Index '{self.index_name}' doesn't exist. Creating...\")\n",
    "            self.create_index()\n",
    "        else:\n",
    "            print(f\"Index '{self.index_name}' already exists\")\n",
    "    \n",
    "    def create_index(self):\n",
    "        \"\"\"Create Elasticsearch index with optimized mapping\"\"\"\n",
    "        mapping = {\n",
    "            \"settings\": {\n",
    "                \"number_of_shards\": 1,\n",
    "                \"number_of_replicas\": 1,\n",
    "                \"analysis\": {\n",
    "                    \"analyzer\": {\n",
    "                        \"email_analyzer\": {\n",
    "                            \"type\": \"custom\",\n",
    "                            \"tokenizer\": \"standard\",\n",
    "                            \"filter\": [\"lowercase\", \"stop\", \"snowball\"]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"email_id\": {\"type\": \"keyword\"},\n",
    "                    \"thread_id\": {\"type\": \"keyword\"},\n",
    "                    \"sender\": {\n",
    "                        \"properties\": {\n",
    "                            \"name\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\"}}},\n",
    "                            \"email\": {\"type\": \"keyword\"}\n",
    "                        }\n",
    "                    },\n",
    "                    \"recipients\": {\"type\": \"keyword\"},\n",
    "                    \"cc\": {\"type\": \"keyword\"},\n",
    "                    \"bcc\": {\"type\": \"keyword\"},\n",
    "                    \"subject\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"email_analyzer\",\n",
    "                        \"fields\": {\"keyword\": {\"type\": \"keyword\"}}\n",
    "                    },\n",
    "                    \"body\": {\n",
    "                        \"properties\": {\n",
    "                            \"plain\": {\"type\": \"text\", \"analyzer\": \"email_analyzer\"},\n",
    "                            \"html\": {\"type\": \"text\", \"index\": False}\n",
    "                        }\n",
    "                    },\n",
    "                    \"snippet\": {\"type\": \"text\"},\n",
    "                    \"date\": {\"type\": \"date\"},\n",
    "                    \"processed_at\": {\"type\": \"date\"},\n",
    "                    \"attachments\": {\n",
    "                        \"type\": \"nested\",\n",
    "                        \"properties\": {\n",
    "                            \"filename\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\"}}},\n",
    "                            \"mime_type\": {\"type\": \"keyword\"},\n",
    "                            \"size\": {\"type\": \"long\"},\n",
    "                            \"has_content\": {\"type\": \"boolean\"}\n",
    "                        }\n",
    "                    },\n",
    "                    \"has_attachments\": {\"type\": \"boolean\"},\n",
    "                    \"attachment_count\": {\"type\": \"integer\"},\n",
    "                    \"labels\": {\"type\": \"keyword\"},\n",
    "                    \"category\": {\"type\": \"keyword\"},\n",
    "                    \"priority\": {\"type\": \"keyword\"},\n",
    "                    \"is_read\": {\"type\": \"boolean\"},\n",
    "                    \"is_important\": {\"type\": \"boolean\"},\n",
    "                    \"is_starred\": {\"type\": \"boolean\"},\n",
    "                    \"extracted_data\": {\"type\": \"object\", \"enabled\": True},\n",
    "                    \"action_items\": {\n",
    "                        \"type\": \"nested\",\n",
    "                        \"properties\": {\n",
    "                            \"description\": {\"type\": \"text\"},\n",
    "                            \"priority\": {\"type\": \"keyword\"},\n",
    "                            \"due_date\": {\"type\": \"date\"},\n",
    "                            \"type\": {\"type\": \"keyword\"},\n",
    "                            \"completed\": {\"type\": \"boolean\"}\n",
    "                        }\n",
    "                    },\n",
    "                    \"action_count\": {\"type\": \"integer\"},\n",
    "                    \"processing_status\": {\"type\": \"keyword\"},\n",
    "                    \"processing_errors\": {\"type\": \"text\"},\n",
    "                    \"embedding\": {\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": 1536,\n",
    "                        \"index\": True,\n",
    "                        \"similarity\": \"cosine\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.es.indices.create(index=self.index_name, body=mapping)\n",
    "        print(f\"Created index: {self.index_name}\")\n",
    "    \n",
    "    def search(self, query: Dict) -> List[Dict]:\n",
    "        \"\"\"Execute a search query\"\"\"\n",
    "        result = self.es.search(index=self.index_name, body=query)\n",
    "        return [hit['_source'] for hit in result['hits']['hits']]\n",
    "\n",
    "print(\"classes loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8a584a-a69e-4f7c-a5ef-95eddbca3ade",
   "metadata": {},
   "source": [
    "Ran this in terminal: \n",
    "\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    --name elasticsearch \\\n",
    "    -m 4GB \\\n",
    "    -p 9200:9200 \\\n",
    "    -p 9300:9300 \\\n",
    "    -e \"discovery.type=single-node\" \\\n",
    "    -e \"xpack.security.enabled=false\" \\\n",
    "    -v es9_data:/usr/share/elasticsearch/data \\\n",
    "    docker.elastic.co/elasticsearch/elasticsearch:9.1.1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ebb339e-4def-49f3-a396-d25b8bcce574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': '1576000f09ac', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'X2EE6hfCRtyf9yI4QAjjnw', 'version': {'number': '9.1.1', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '5e94055934defa56e454868b7783b2a3b683785e', 'build_date': '2025-08-05T01:07:31.959947279Z', 'build_snapshot': False, 'lucene_version': '10.2.2', 'minimum_wire_compatibility_version': '8.19.0', 'minimum_index_compatibility_version': '8.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es_client = Elasticsearch('http://localhost:9200')\n",
    "\n",
    "es_client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "212623cb-b069-473f-a9b8-b1af37fef3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Index 'emails' already exists\n",
      "Index name: emails\n"
     ]
    }
   ],
   "source": [
    "# Initialize ElasticSearchEmailStore\n",
    "\n",
    "es_store = ElasticsearchEmailStore(\n",
    "    es_host=\"localhost\",\n",
    "    es_port=9200,\n",
    "    index_name=\"emails\",\n",
    ")\n",
    "\n",
    "print(f\"Index name: {es_store.index_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914f5e3-2c87-474f-90fd-31a5fe3935eb",
   "metadata": {},
   "source": [
    "# Email Fetching"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
