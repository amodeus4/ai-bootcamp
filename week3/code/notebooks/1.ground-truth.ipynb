{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e22122-186d-4912-9404-7377a7d8092d",
   "metadata": {},
   "source": [
    "### purpose of this is to generate synthetic data (week 3 - lesson 1 offline evaluation agents)\n",
    "\n",
    "we'll create a system to automatically generate realistic user questions based on the technical documentation usecase.This synthetic data can be used for testing our agent and its search function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24eebb0b-8bc9-4218-a417-623c83fde3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "367762c5-2988-4418-a20f-50321731220e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import docs\n",
    "\n",
    "raw_documents = docs.read_github_data()\n",
    "documents = docs.parse_data(raw_documents)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576b3e3-59ed-4d14-a4b4-bd3246cf9fe6",
   "metadata": {},
   "source": [
    "If for your case the data is too big (e.g. it's a book), then use some logical-based chunking - e.g. per chapter/section, and then use them for creating questions.\n",
    "In this case, we get 95 documents total. Now let's filter and select documents that are suitable for question generation.\n",
    "\n",
    "\n",
    "This filtering process:\n",
    "- Skips documents without titles\n",
    "- Excludes unpublished, legacy, and leftover content\n",
    "- Only includes substantial documents (over 1000 characters)\n",
    "\n",
    "The longer the document, the more questions we may want to generate. So let's use the following logic: for each 1000 characters of the document we generate one question.\n",
    "\n",
    "This way we can generate approximately 471 questions from 69 selected documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33dc592f-cb6f-4337-90cc-cb0e298f266b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data definition\n",
      "11046 11\n",
      "------------\n",
      "Descriptors\n",
      "12588 12\n",
      "------------\n",
      "Overview\n",
      "3231 3\n",
      "------------\n",
      "Metric generators\n",
      "2949 2\n",
      "------------\n",
      "Output formats\n",
      "1584 1\n",
      "------------\n",
      "Introduction\n",
      "22069 22\n",
      "------------\n",
      "Report\n",
      "4989 4\n",
      "------------\n",
      "Add tags and metadata\n",
      "2340 2\n",
      "------------\n",
      "Tests\n",
      "9154 9\n",
      "------------\n",
      "Alerts\n",
      "1282 1\n",
      "------------\n",
      "Add dashboard panels (API)\n",
      "13004 13\n",
      "------------\n",
      "Add dashboard panels (UI)\n",
      "4258 4\n",
      "------------\n",
      "Overview\n",
      "2735 2\n",
      "------------\n",
      "Overview\n",
      "2216 2\n",
      "------------\n",
      "Work with datasets\n",
      "2114 2\n",
      "------------\n",
      "Run evals via API\n",
      "2162 2\n",
      "------------\n",
      "Explore view\n",
      "1899 1\n",
      "------------\n",
      "No code evals\n",
      "4377 4\n",
      "------------\n",
      "Overview\n",
      "2138 2\n",
      "------------\n",
      "Batch monitoring\n",
      "2384 2\n",
      "------------\n",
      "Overview\n",
      "3768 3\n",
      "------------\n",
      "Introduction\n",
      "2408 2\n",
      "------------\n",
      "Manage Projects\n",
      "4614 4\n",
      "------------\n",
      "Overview\n",
      "1392 1\n",
      "------------\n",
      "Overview\n",
      "1507 1\n",
      "------------\n",
      "Set up tracing\n",
      "10120 10\n",
      "------------\n",
      "Evidently Cloud\n",
      "1218 1\n",
      "------------\n",
      "Self-hosting\n",
      "5515 5\n",
      "------------\n",
      "Evidently and GitHub actions\n",
      "1375 1\n",
      "------------\n",
      "LLM evaluations\n",
      "2314 2\n",
      "------------\n",
      "LLM as a judge\n",
      "21834 21\n",
      "------------\n",
      "LLM-as-a-jury\n",
      "9235 9\n",
      "------------\n",
      "RAG evals\n",
      "13227 13\n",
      "------------\n",
      "LLM regression testing\n",
      "21712 21\n",
      "------------\n",
      "Tutorials and guides\n",
      "12043 12\n",
      "------------\n",
      "Evidently Cloud v2\n",
      "1897 1\n",
      "------------\n",
      "Migration Guide\n",
      "7621 7\n",
      "------------\n",
      "Open-source vs. Cloud\n",
      "6013 6\n",
      "------------\n",
      "Telemetry\n",
      "10449 10\n",
      "------------\n",
      "Why Evidently?\n",
      "4876 4\n",
      "------------\n",
      "What is Evidently?\n",
      "1725 1\n",
      "------------\n",
      "All Descriptors\n",
      "31874 31\n",
      "------------\n",
      "All Metrics\n",
      "54996 54\n",
      "------------\n",
      "Overview\n",
      "1140 1\n",
      "------------\n",
      "Customize Data Drift\n",
      "17825 17\n",
      "------------\n",
      "Custom Text Descriptor\n",
      "3569 3\n",
      "------------\n",
      "Use HuggingFace models\n",
      "10737 10\n",
      "------------\n",
      "Configure LLM Judges\n",
      "26737 26\n",
      "------------\n",
      "Custom Metric\n",
      "4162 4\n",
      "------------\n",
      "Classification metrics\n",
      "8031 8\n",
      "------------\n",
      "Data stats and quality\n",
      "7536 7\n",
      "------------\n",
      "Data drift\n",
      "8366 8\n",
      "------------\n",
      "Ranking and RecSys metrics\n",
      "10174 10\n",
      "------------\n",
      "Regression metrics\n",
      "9333 9\n",
      "------------\n",
      "Classification\n",
      "3972 3\n",
      "------------\n",
      "Data Drift\n",
      "5670 5\n",
      "------------\n",
      "Data Summary\n",
      "3898 3\n",
      "------------\n",
      "Recommendations\n",
      "3120 3\n",
      "------------\n",
      "Regression\n",
      "3473 3\n",
      "------------\n",
      "Text Evals\n",
      "3373 3\n",
      "------------\n",
      "LLM Evaluation\n",
      "9621 9\n",
      "------------\n",
      "Data and ML checks\n",
      "6907 6\n",
      "------------\n",
      "Tracing\n",
      "5942 5\n",
      "------------\n",
      "Adversarial testing\n",
      "2514 2\n",
      "------------\n",
      "Create synthetic inputs\n",
      "1934 1\n",
      "------------\n",
      "Synthetic data\n",
      "1274 1\n",
      "------------\n",
      "RAG evaluation dataset\n",
      "1982 1\n",
      "------------\n",
      "Why synthetic data?\n",
      "2082 2\n",
      "------------\n",
      "471\n"
     ]
    }
   ],
   "source": [
    "num_questions_total = 0\n",
    "\n",
    "selected_documents = []\n",
    "\n",
    "for doc in documents[5:]:\n",
    "    if 'title' not in doc:\n",
    "        continue\n",
    "\n",
    "    title = doc['title']\n",
    "    if 'unpublished' in title.lower():\n",
    "        continue\n",
    "    if 'legacy' in title.lower():\n",
    "        continue\n",
    "    if 'leftovers' in title.lower():\n",
    "        continue\n",
    "\n",
    "    content = doc.get('content', '').strip()\n",
    "    if len(content) <= 1000:\n",
    "        continue\n",
    "\n",
    "    num_questions = len(content) // 1000\n",
    "    print(doc.get('title'))\n",
    "    print(len(content), num_questions)\n",
    "    num_questions_total = num_questions_total + num_questions\n",
    "    print('------------')\n",
    "\n",
    "    selected_documents.append(doc)\n",
    "\n",
    "print(num_questions_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec6e1906-7199-4b08-88c2-7e2d4b066638",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When working on an AI system, you need test data to run automated evaluations for quality and safety. A test dataset is a structured set of test cases. It can contain:\n",
      "\n",
      "* Just the inputs, or\n",
      "* Both inputs and expected outputs (ground truth).\n",
      "\n",
      "You can use this test dataset to:\n",
      "\n",
      "* Run **experiments** and track if changes improve or degrade system performance.\n",
      "* Run **regression testing** to ensure updates don’t break what was already working.\n",
      "* **Stress-test** your system with complex or adversarial inputs to check its resilience.\n",
      "\n",
      "![](/images/synthetic/synthetic_experiments_img.png)\n",
      "\n",
      "You can create test datasets manually, collect them from real or historical data, or generate them synthetically. While real data is best, it is not always available or sufficient to cover all cases. Public LLM benchmarks help with general model comparisons but don’t reflect your specific use case. Manually writing test cases takes time and effort.\n",
      "\n",
      "**Synthetic data helps here**. It’s especially useful when you are:\n",
      "\n",
      "* You're starting from scratch and don’t have real data.\n",
      "* You need to scale a manually designed dataset with more variation.\n",
      "* You want to test edge cases, adversarial inputs, or system robustness.\n",
      "* You're evaluating complex AI systems like RAG and AI agents.\n",
      "\n",
      "![](/images/synthetic/synthetic_adversarial_img.png)\n",
      "\n",
      "Synthetic data is not a replacement for real data or expert-designed tests — it’s a way to add variety and speed up the process. With synthetic data you can:\n",
      "\n",
      "* Quickly generate hundreds structured test cases.\n",
      "* Fill gaps by adding missing scenarios and tricky inputs.\n",
      "* Create controlled variations to evaluate specific weaknesses.\n",
      "\n",
      "It’s a practical way to expand your evaluation dataset efficiently while keeping human expertise focused on high-value testing.\n",
      "\n",
      "Synthetic data can also work for **complex AI systems** where designing test cases is simply difficult. For example, in RAG evaluation, synthetic data helps create input-output datasets from knowledge bases. In AI agent testing, it enables multi-turn interactions across different scenarios.\n"
     ]
    }
   ],
   "source": [
    "print(doc['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b145c4-4f5e-45d6-a07f-22bd435a312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for structured output\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm_structured(instructions, user_prompt, output_format, model=\"gpt-4o-mini\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.parse(\n",
    "        model=model,\n",
    "        input=messages,\n",
    "        text_format=output_format\n",
    "    )\n",
    "\n",
    "    return (response.output_parsed, response.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "177b5109-90e4-4ff2-8d0a-10d50cbd6a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include intent classification to understand whether users are looking for conceptual explanations or code examples:\n",
    "\n",
    "\n",
    "generator_instructions = \"\"\"\n",
    "You are given a technical article. Your task is to imagine what a person might type into a search engine \n",
    "before finding and reading this article.\n",
    "\n",
    "Generate realistic, human-like search queries — not formal questions. \n",
    "They should sound like what people actually type into Google or Stack Overflow \n",
    "when trying to solve a problem, learn a concept, or find code examples.\n",
    "\n",
    "Guidelines:\n",
    "- Avoid full-sentence questions with punctuation like \"What is...\" or \"How do I...\".\n",
    "- Use short, natural search phrases instead, such as:\n",
    "  - \"evidently data definition example\"\n",
    "  - \"map target and prediction columns evidently\"\n",
    "  - \"difference between timestamp and datetime evidently\"\n",
    "- Make queries varied and spontaneous, not repetitive or over-polished.\n",
    "- Assume users of different knowledge levels:\n",
    "  - beginner: broad or basic understanding\n",
    "  - intermediate: knows basic terms but seeks clarification or examples\n",
    "  - advanced: familiar with the tool, looking for details, edge cases, or integration options\n",
    "\n",
    "Distribution rules:\n",
    "- 60% of the queries should target beginner-level users\n",
    "- 30% should target intermediate-level users\n",
    "- 10% should target advanced-level users\n",
    "- 75% of queries should have an intent of \"code\" (looking for examples or implementation)\n",
    "- 25% should have an intent of \"text\" (looking for conceptual or theoretical explanations)\n",
    "\n",
    "For each generated query, include:\n",
    "- question: the natural, human-style search phrase\n",
    "- summary_answer: a short 1–2 sentence summary of how the article addresses it\n",
    "- difficulty: one of [\"beginner\", \"intermediate\", \"advanced\"]\n",
    "- intent: one of [\"text\", \"code\"]\n",
    "\n",
    "Also include a description summarizing what kind of article the questions are about.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8eff159c-3e86-4d53-8547-1e8d32c175f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "    summary_answer: str\n",
    "\n",
    "class GeneratedQuestions(BaseModel):\n",
    "    questions: list[Question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41bae3d1-82be-47a0-becf-387f77067db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def process_document(doc):\n",
    "    content = doc['content']\n",
    "    num_questions = len(content) // 1000\n",
    "    user_prompt = f\"\"\"generate {num_questions} for this document:\n",
    "    <document>{json.dumps(doc)}</document>\n",
    "    \"\"\"\n",
    "    response, usage = llm_structured(\n",
    "        instructions=generator_instructions,\n",
    "        user_prompt=user_prompt,\n",
    "        output_format=GeneratedQuestions\n",
    "    )\n",
    "    return {\n",
    "        'doc': doc,\n",
    "        'questions': response.questions,\n",
    "        'usage': usage\n",
    "    }\n",
    "\n",
    "doc = selected_documents[0]\n",
    "result = process_document(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e25fa11-f652-4847-903f-ec25eb940c89",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc': {'title': 'Data definition', 'description': 'How to map the input data.', 'content': 'To run evaluations, you must create a `Dataset` object with a `DataDefinition`, which maps:\\n\\n- **Column types** (e.g., categorical, numerical, text).\\n- **Column roles** (e.g., id, prediction, target).\\n\\nThis allows Evidently to process the data correctly. Some evaluations need specific columns and will fail if they\\'re missing. You can define the mapping using the Python API or by assigning columns visually when uploading data to the Evidently platform.\\n\\n## Basic flow\\n\\n**Step 1. Imports.** Import the following modules:\\n\\n```python\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\n```\\n\\n**Step 2. Prepare your data.** Use a pandas.DataFrame.\\n\\n<Info>\\n  Your data can have [flexible structure](/docs/library/overview#dataset) with any mix of categorical, numerical or text columns. Check the [Reference table](/metrics/all_metrics) for data requirements in specific evaluations.\\n</Info>\\n\\n**Step 3. Create a Dataset object**. Use `Dataset.from_pandas` with `data_definition`:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=DataDefinition()\\n)\\n```\\n\\nTo map columns automatically, pass an empty `DataDefinition()` . Evidently will map columns:\\n\\n- By type (numerical, categorical).\\n- By matching column names to roles (e.g., a column \"target\" treated as target).\\n\\nAutomation works in many cases, but manual mapping is more accurate. It is also necessary for evaluating prediction quality or handling text columns.\\n\\n<Note>\\n  **How to set the data definition manually?** See the section below for available options.\\n</Note>\\n\\n**Step 4. Run evals.** Once the **Dataset** object is ready, you can [add Descriptors ](/docs/library/descriptors) and[ run Reports](/docs/library/report).\\n\\n### Special cases\\n\\n**Working directly with pandas.DataFrame**. You can sometimes pass a `pandas.DataFrame` directly to `report.run()` without creating the Dataset object. This works for checks like numerical/categorical data summaries or drift detection. However, it\\'s best to always create a `Dataset` object explicitly for clarity and control.\\n\\n**Working with two datasets**. If you\\'re working with current and reference datasets (e.g., for drift detection), create a Dataset object for each. Both must have identical data definition.\\n\\n## Data definition\\n\\nThis page shows all the different mapping options. Note that you **only need to use the relevant ones** that apply for your evaluation scenario. For example, you don’t need columns like target/prediction to run data quality or LLM checks.\\n\\n### Column types\\n\\nKnowing the column type helps compute correct statistics, visualizations, and pick default tests.\\n\\n#### Text data\\n\\nIf you run LLM evaluations, simply specify the columns with inputs/outputs as text.\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\n<Info>\\n  **It\\'s optional but useful**. You can [generate text descriptors](/docs/library/descriptors) without explicit mapping. But it\\'s a good idea to map text columns since you may later run other evals which vary by column type.\\n</Info>\\n\\n#### Tabular data\\n\\nMap numerical, categorical or datetime columns:\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"],\\n    numerical_columns=[\"Age\", \"Salary\"],\\n    categorical_columns=[\"Department\"],\\n    datetime_columns=[\"Joining_Date\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\nExplicit mapping helps avoid mistakes like misclassifying numerical columns with few unique values as categorical.\\n\\n<Info>\\n  If you **exclude** certain columns in mapping, they’ll be ignored in all evaluations.\\n</Info>\\n\\n#### Default column types\\n\\nIf you do not pass explicit mapping, the following defaults apply:\\n\\n| **Column Type**       | **Description**                                                                                                                       | **Automated Mapping**                               |\\n| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |\\n| `numerical_columns`   | <ul>      <li>      Columns with numeric values.</li>            </ul>                                                                | All columns with numeric types (`np.number`).       |\\n| `datetime_columns`    | <ul>      <li>      Columns with datetime values.</li>            <li>      Ignored in data drift calculations.</li>            </ul> | All columns with DateTime format (`np.datetime64`). |\\n| `categorical_columns` | <ul>      <li>      Columns with categorical values.</li>            </ul>                                                            | All non-numeric/non-datetime columns.               |\\n| `text_columns`        | <ul>      <li>      Text columns.</li>            <li>      Mapping required for text data drift detection.</li>            </ul>     | No automated mapping.                               |\\n\\n### ID and timestamp\\n\\nIf you have a timestamp or ID column, it\\'s useful to identify them.\\n\\n```python\\ndefinition = DataDefinition(\\n    id_column=\"Id\",\\n    timestamp=\"Date\"\\n    )\\n```\\n\\n| **Column role** | **Description**                                                                                                             | **Automated mapping**    |\\n| --------------- | --------------------------------------------------------------------------------------------------------------------------- | ------------------------ |\\n| `id_column`     | <ul>      <li>      Identifier column.</li>            <li>      Ignored in data drift calculations.</li>            </ul>  | Column named \"id\"        |\\n| `timestamp`     | <ul>      <li>      Timestamp column.</li>            <li>       Ignored in data drift calculations. </li>            </ul> | Column named \"timestamp\" |\\n\\n<Info>\\n  How is`timestamp` different from `datetime_columns`?\\n\\n  - **DateTime** is a column type. You can have many DateTime columns in the dataset. For example, conversation start / end time or features like \"date of last contact.\"\\n  - **Timestamp** is a role. You can have a single timestamp column. It often represents the time when a data input was recorded. Use it if you want to see it as index on the plots.\\n</Info>\\n\\n### LLM evals\\n\\nWhen you generate [text descriptors](/docs/library/descriptors) and add them to the dataset, they are automatically mapped as `descriptors` in Data Definition. This means they will be included in the `TextEvals` [preset](/metrics/preset_text_evals) or treated as descriptors when you plot them on the dashboard.\\n\\nHowever, if you computed some scores or metadata externally and want to treat them as descriptors, you can map them explicitly:\\n\\n```python\\ndefinition = DataDefinition(\\n    numerical_descriptors=[\"chat_length\", \"user_rating\"],\\n    categorical_descriptors=[\"upvotes\", \"model_type\"]\\n    )\\n```\\n\\n### Regression\\n\\nTo run regression quality checks, you must map the columns with:\\n\\n- Target: actual values.\\n- Prediction: predicted values.\\n\\nYou can have several regression results in the dataset, for example in case of multiple regression. (Pass the mappings in a list).\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    regression=[Regression(target=\"y_true\", prediction=\"y_pred\")]\\n    )\\n```\\n\\nDefaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction: str = \"prediction\"\\n```\\n\\n### Classification\\n\\nTo run classification checks, you must map the columns with:\\n\\n- Target: true label.\\n- Prediction: predicted labels/probabilities.\\n\\nThere two different mapping options, for binary and multi-class classification. You can also have several classification results in the dataset. (Pass the mappings in a list).\\n\\n#### Multiclass\\n\\nExample mapping:\\n\\n```python\\nfrom evidently import MulticlassClassification\\n\\ndata_def = DataDefinition(\\n    classification=[MulticlassClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\",\\n        prediction_probas=[\"0\", \"1\", \"2\"],  # If probabilistic classification\\n        labels={\"0\": \"class_0\", \"1\": \"class_1\", \"2\": \"class_2\"}  # Optional, for display only\\n    )]\\n)\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction_labels: str = \"prediction\"\\n    prediction_probas: Optional[List[str]] = None #if probabilistic classification\\n    labels: Optional[Dict[Label, str]] = None\\n```\\n\\n<Note>\\n  When you have multiclass classification with predicted probabilities in separate columns, the column names in `prediction_probas` must exactly match the class labels. For example, if your classes are 0, 1, and 2, your probability columns must be named: \"0\", \"1\", \"2\". Values in `target` and `prediction` columns should be strings.\\n</Note>\\n\\n#### Binary\\n\\nExample mapping:\\n\\n```python\\nfrom evidently import BinaryClassification\\n\\ndefinition = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\")],\\n    categorical_columns=[\"target\", \"prediction\"])\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction_labels: Optional[str] = None\\n    prediction_probas: Optional[str] = \"prediction\" #if probabilistic classification\\n    pos_label: Label = 1 #name of the positive label\\n    labels: Optional[Dict[Label, str]] = None\\n```\\n\\n### Ranking\\n\\n#### RecSys\\n\\nTo evaluate recommender systems performance, you must map the columns with:\\n\\n- Prediction: this could be predicted score or rank.\\n- Target: relevance labels (e.g., this could be an interaction result like user click or upvote, or a true relevance label)\\n\\nThe **target** column can contain either:\\n\\n- a binary label (where `1` is a positive outcome)\\n- any scores (positive values, where a higher value corresponds to a better match or a more valuable user action).\\n\\nHere are the examples of the expected data inputs.\\n\\nIf the system prediction is a **score** (expected by default):\\n\\n| user_id | item_id | prediction (score) | target (relevance) |\\n| ------- | ------- | ------------------ | ------------------ |\\n| user_1  | item_1  | 1.95               | 0                  |\\n| user_1  | item_2  | 0.8                | 1                  |\\n| user_1  | item_3  | 0.05               | 0                  |\\n\\nIf the model prediction is a **rank**:\\n\\n| user_id | item_id | prediction (rank) | target (relevance) |\\n| ------- | ------- | ----------------- | ------------------ |\\n| user_1  | item_1  | 1                 | 0                  |\\n| user_1  | item_2  | 2                 | 1                  |\\n| user_1  | item_3  | 3                 | 0                  |\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    ranking=[Recsys()]\\n    )\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    user_id: str = \"user_id\" #columns with user IDs\\n    item_id: str = \"item_id\" #columns with ranked items\\n    target: str = \"target\"\\n    prediction: str = \"prediction\"\\n```', 'filename': 'docs/library/data_definition.mdx'}, 'questions': [Question(question='mapping input data in Evidently', summary_answer='The article explains how to create a `Dataset` object with a `DataDefinition` that maps column types and roles for correct data processing in Evidently evaluations. It emphasizes the importance of proper mapping for successful evaluations and provides examples for implementation using Python API and visual methods when uploading data to the platform, aiding beginners in initial setup for evaluations in Evidently tools and workflows, focusing on a clear process to achieve accurate mappings using provided code snippets and guidance.'), Question(question='column types for data definition', summary_answer='The article details the types of columns that can be defined in the `DataDefinition`, such as categorical, numerical, text, and datetime, highlighting their importance for evaluations and suggesting default mappings for ease of use.'), Question(question='how to create Dataset in Evidently', summary_answer=\"To create a Dataset in Evidently, you need to use `Dataset.from_pandas()` along with a `DataDefinition` that specifies the mapping of your input data's columns, as shown in the provided code examples.\"), Question(question='Evidently Dataset code example', summary_answer='The article includes code snippets demonstrating how to create a `Dataset` using `Dataset.from_pandas()` and how to define column types and roles using the `DataDefinition` class.'), Question(question='defining text columns in DataDefinition', summary_answer=\"You can specify text columns in the `DataDefinition` for tasks like LLM evaluations, ensuring that evaluations consider the correct input/output types as demonstrated in the article's examples.\"), Question(question='using DataDefinition to avoid errors', summary_answer='Explicitly mapping your columns with `DataDefinition` can help avoid common mistakes, such as misclassifying data types, which can affect the results of evaluations, as discussed in the article.'), Question(question='Evidently DataDefinition manual mapping steps', summary_answer='The article provides steps for manually mapping data using `DataDefinition`, including examples for defining different column types and roles critical for effective evaluations.'), Question(question='automated vs manual mapping in Evidently', summary_answer='It discusses the benefits and downsides of automated mapping versus manual mapping when creating a `Dataset`, noting that while automation is convenient, manual mapping provides better accuracy for critical evaluations.'), Question(question='how to evaluate regression with DataDefinition', summary_answer='For regression evaluations, the article outlines how to map target and prediction columns using `DataDefinition`, including example code for effective implementation.'), Question(question='Evidently classification checks example', summary_answer='The article provides detailed code examples for mapping columns for classification checks, including both binary and multiclass classifications, to ensure accurate evaluation results.'), Question(question='default mappings in DataDefinition', summary_answer='The article lists default mappings for various column types within `DataDefinition`, which can be automatically applied if no explicit mappings are provided.')], 'usage': ResponseUsage(input_tokens=3154, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=580, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=3734)}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96495d6-928c-4bdd-a657-bca84f98b8a5",
   "metadata": {},
   "source": [
    "Final schema with intent classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0034a484-83da-42a8-9d1b-5c056f51e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "\n",
    "class Question(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a realistic search-engine-style query a user might type before finding the article.\n",
    "    Each question captures the likely search phrase, a short summary answer,\n",
    "    the user's assumed skill level, and their intent (conceptual or code-focused).\n",
    "    \"\"\"\n",
    "    question: str = Field(\n",
    "        ...,\n",
    "        description=\"A natural, short search query — not a full-sentence question — phrased like something typed into Google.\"\n",
    "    )\n",
    "    summary_answer: str = Field(\n",
    "        ...,\n",
    "        description=\"A concise 1–2 sentence summary of how the article addresses the query.\"\n",
    "    )\n",
    "    difficulty: Literal[\"beginner\", \"intermediate\", \"advanced\"] = Field(\n",
    "        ...,\n",
    "        description=\"The assumed knowledge level of the user making the query.\"\n",
    "    )\n",
    "    intent: Literal[\"text\", \"code\"] = Field(\n",
    "        ...,\n",
    "        description=\"Specifies if the user's intent is to get a theoretical explanation ('text') or an implementation example ('code').\"\n",
    "    )\n",
    "\n",
    "\n",
    "class GeneratedQuestions(BaseModel):\n",
    "    \"\"\"\n",
    "    A structured collection of human-like search queries derived from a given article.\n",
    "    Includes a brief description of the article topic and a list of generated queries.\n",
    "    Difficulty distribution: 60% beginner, 30% intermediate, 10% advanced.\n",
    "    Intent distribution: 75% code-focused, 25% concept-focused.\n",
    "    \"\"\"\n",
    "    description: str = Field(\n",
    "        ...,\n",
    "        description=\"A summary of the article or topic these search-style questions were generated for.\"\n",
    "    )\n",
    "    questions: List[Question] = Field(\n",
    "        ...,\n",
    "        description=\"A list of realistic search queries with short summaries, difficulty levels, and user intent.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf3cca8-ddca-452b-87cf-84dbac93f9a2",
   "metadata": {},
   "source": [
    "### Document processing\n",
    "\n",
    "We want to make it fast, so we'll do this in multiple parallel threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2d3f734-39cb-4a9b-9b1c-be52e619d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def map_progress(pool, seq, f):\n",
    "    \"\"\"Map function f over seq using the provided executor pool while\n",
    "    displaying a tqdm progress bar. Returns a list of results in submission order.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    with tqdm(total=len(seq)) as progress:\n",
    "        futures = []\n",
    "    \n",
    "        for el in seq:\n",
    "            future = pool.submit(f, el)\n",
    "            future.add_done_callback(lambda p: progress.update())\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b7dd68a-330e-495f-a09a-d08ff104017b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 68/68 [01:39<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor(max_workers=6) as pool:\n",
    "    all_results = map_progress(pool, selected_documents, process_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f66801-7d3b-4896-9aef-f88f6a523536",
   "metadata": {},
   "source": [
    "### Check how much it cost to generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6df07575-f248-4921-846e-8ac4254ee71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost: $0.0405\n"
     ]
    }
   ],
   "source": [
    "input_cost_per_1k = 0.00015\n",
    "output_cost_per_1k = 0.0006\n",
    "\n",
    "total_input = 0\n",
    "total_output = 0\n",
    "\n",
    "for res in all_results:\n",
    "    usage = res['usage']\n",
    "    total_input += usage.input_tokens\n",
    "    total_output += usage.output_tokens\n",
    "\n",
    "cost = (total_input / 1000 * input_cost_per_1k) + (total_output / 1000 * output_cost_per_1k)\n",
    "print(f\"Total cost: ${cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2513a58f-ef90-4c43-a402-11f829a2b049",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d285606-ab28-4b85-a21e-9224e83e4312",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions = []\n",
    "\n",
    "for res in all_results:\n",
    "    doc = res['doc']\n",
    "    questions = res['questions']\n",
    "    for q in questions:\n",
    "        q_dict = q.model_dump()\n",
    "        q_dict['filename'] = doc['filename']\n",
    "        all_questions.append(q_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38df8a32-e68b-480b-97ad-59947473b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_questions = pd.DataFrame(all_questions)\n",
    "df_questions.to_csv('ground_truth_evidently.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7891518-d9a9-4634-b151-73bd8ed9872f",
   "metadata": {},
   "source": [
    "## Evaluation retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e769c-69ac-4d82-bdbf-4f411f237839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1768c0bf-a1ca-4c4d-95e2-9fc619ee4e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
