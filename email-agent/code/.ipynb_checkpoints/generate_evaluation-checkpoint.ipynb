{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27213753",
   "metadata": {},
   "source": [
    "# Email Agent Evaluation: Agent-Generated Questions\n",
    "\n",
    "This notebook implements the Week 3 approach for generating and evaluating email agent responses.\n",
    "\n",
    "## Process Overview\n",
    "1. **Generate Questions**: Use LLM to generate realistic test questions from agent documentation\n",
    "2. **Run Evaluation**: Execute agent on each question and collect responses  \n",
    "3. **Evaluate Responses**: Score responses using 8-point rubric\n",
    "4. **Analyze Results**: Review metrics and identify improvement areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4737976",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af4dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add email_agent to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411e7564",
   "metadata": {},
   "source": [
    "## Section 2: Generate Synthetic Email Data\n",
    "\n",
    "Create realistic test emails based on your use case. These represent the data your agent will work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define synthetic email dataset for testing\n",
    "# Based on typical business email scenarios: invoices, project updates, HR communications\n",
    "\n",
    "synthetic_emails = [\n",
    "    {\n",
    "        'from': 'accounting@luxcmar.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'Invoice #INV-2025-001 for January Services',\n",
    "        'body': 'Please find attached invoice for services rendered in January. Total amount due: $5,250. Payment terms: Net 30 days.',\n",
    "        'date': (datetime.now() - timedelta(days=5)).isoformat(),\n",
    "        'has_attachments': True,\n",
    "        'attachments': ['invoice_january.pdf']\n",
    "    },\n",
    "    {\n",
    "        'from': 'accounting@luxcmar.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'Invoice #INV-2025-002 for February Services',\n",
    "        'body': 'Please find attached invoice for February services. Total: $7,500. Please remit payment within 30 days.',\n",
    "        'date': (datetime.now() - timedelta(days=3)).isoformat(),\n",
    "        'has_attachments': True,\n",
    "        'attachments': ['invoice_february.pdf']\n",
    "    },\n",
    "    {\n",
    "        'from': 'john.smith@company.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'RE: Project Alpha Status Update',\n",
    "        'body': 'The project is on schedule. We completed 60% of Phase 1. Team will present findings next week.',\n",
    "        'date': (datetime.now() - timedelta(days=2)).isoformat(),\n",
    "        'has_attachments': False,\n",
    "        'attachments': []\n",
    "    },\n",
    "    {\n",
    "        'from': 'john.smith@company.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'Project Alpha - Technical Specifications',\n",
    "        'body': 'Attached are the technical specs for Project Alpha. Please review and provide feedback by Friday.',\n",
    "        'date': (datetime.now() - timedelta(days=1)).isoformat(),\n",
    "        'has_attachments': True,\n",
    "        'attachments': ['project_alpha_specs.pdf', 'requirements.docx']\n",
    "    },\n",
    "    {\n",
    "        'from': 'hr@company.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'Benefits Renewal - Action Required',\n",
    "        'body': 'Your benefits renewal period is open. Please review available plans and enroll by March 31st.',\n",
    "        'date': (datetime.now() - timedelta(days=7)).isoformat(),\n",
    "        'has_attachments': True,\n",
    "        'attachments': ['benefits_guide_2025.pdf']\n",
    "    },\n",
    "    {\n",
    "        'from': 'hr@company.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'RE: Benefits Renewal - Action Required',\n",
    "        'body': 'Hi, just a reminder that enrollment closes on March 31st. Let me know if you have questions.',\n",
    "        'date': (datetime.now() - timedelta(days=5)).isoformat(),\n",
    "        'has_attachments': False,\n",
    "        'attachments': []\n",
    "    },\n",
    "    {\n",
    "        'from': 'sales@vendor.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'Q1 2025 Contract - Review and Sign',\n",
    "        'body': 'Please review and sign the attached Q1 contract. Total value: $25,000 for software licenses.',\n",
    "        'date': (datetime.now() - timedelta(days=10)).isoformat(),\n",
    "        'has_attachments': True,\n",
    "        'attachments': ['contract_q1_2025.pdf']\n",
    "    },\n",
    "    {\n",
    "        'from': 'alice.johnson@project.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'Meeting Notes - Strategic Planning',\n",
    "        'body': 'Please find meeting notes from today\\'s strategic planning session. Action items assigned to team.',\n",
    "        'date': (datetime.now() - timedelta(days=1)).isoformat(),\n",
    "        'has_attachments': True,\n",
    "        'attachments': ['meeting_notes_2025-01-24.docx']\n",
    "    },\n",
    "    {\n",
    "        'from': 'alice.johnson@project.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'RE: Meeting Notes - Strategic Planning',\n",
    "        'body': 'Follow-up: Can you please provide your input on Section 3 by tomorrow?',\n",
    "        'date': (datetime.now() - timedelta(hours=2)).isoformat(),\n",
    "        'has_attachments': False,\n",
    "        'attachments': []\n",
    "    },\n",
    "    {\n",
    "        'from': 'compliance@company.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'Annual Compliance Training - Complete by March 15',\n",
    "        'body': 'All employees must complete annual compliance training. Click link to access course.',\n",
    "        'date': (datetime.now() - timedelta(days=15)).isoformat(),\n",
    "        'has_attachments': False,\n",
    "        'attachments': []\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Generated {len(synthetic_emails)} synthetic emails\")\n",
    "print(\"\\\\nSample emails:\")\n",
    "for i, email in enumerate(synthetic_emails[:3], 1):\n",
    "    print(f\"  {i}. From: {email['from']} | Subject: {email['subject']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41d56f",
   "metadata": {},
   "source": [
    "## Section 3: Create Test Questions Based on Synthetic Data\n",
    "\n",
    "These questions are designed to test specific agent capabilities with the synthetic emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318baae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test questions based on the synthetic data\n",
    "# These test different agent capabilities\n",
    "\n",
    "test_questions = [\n",
    "    {\n",
    "        'id': 'q1',\n",
    "        'question': 'Show me all invoices from accounting@luxcmar.com',\n",
    "        'type': 'sender_search',\n",
    "        'expected_result': 'Should find 2 invoices (INV-2025-001 and INV-2025-002)',\n",
    "        'tests': ['conversation_history', 'sender_filtering']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q2',\n",
    "        'question': 'What is the total amount of invoices?',\n",
    "        'type': 'data_extraction',\n",
    "        'expected_result': 'Should identify $5,250 + $7,500 = $12,750 total',\n",
    "        'tests': ['attachment_parsing', 'content_understanding']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q3',\n",
    "        'question': 'What is the deadline for the HR benefits enrollment?',\n",
    "        'type': 'deadline_extraction',\n",
    "        'expected_result': 'Should find March 31st deadline',\n",
    "        'tests': ['search', 'content_understanding']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q4',\n",
    "        'question': 'Show all emails from john.smith@company.com',\n",
    "        'type': 'sender_search',\n",
    "        'expected_result': 'Should find 2 emails about Project Alpha',\n",
    "        'tests': ['conversation_history', 'threading']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q5',\n",
    "        'question': 'What files did john.smith@company.com attach?',\n",
    "        'type': 'attachment_list',\n",
    "        'expected_result': 'Should list project_alpha_specs.pdf and requirements.docx',\n",
    "        'tests': ['attachment_handling', 'threading']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q6',\n",
    "        'question': 'Find all emails with attachments',\n",
    "        'type': 'filter_by_attachment',\n",
    "        'expected_result': 'Should find 6 emails with attachments',\n",
    "        'tests': ['search', 'attachment_filtering']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q7',\n",
    "        'question': 'What is the contract value with the vendor?',\n",
    "        'type': 'data_extraction',\n",
    "        'expected_result': 'Should find $25,000 contract value',\n",
    "        'tests': ['attachment_parsing', 'search']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q8',\n",
    "        'question': 'Show me the conversation thread with alice.johnson@project.com',\n",
    "        'type': 'thread_conversation',\n",
    "        'expected_result': 'Should show both emails in chronological order (original + follow-up)',\n",
    "        'tests': ['threading', 'conversation_history']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q9',\n",
    "        'question': 'What are the action items from the strategic planning meeting?',\n",
    "        'type': 'action_extraction',\n",
    "        'expected_result': 'Should parse meeting notes and identify action items',\n",
    "        'tests': ['attachment_parsing', 'content_understanding']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q10',\n",
    "        'question': 'Which emails arrived in the last 3 days?',\n",
    "        'type': 'date_filter',\n",
    "        'expected_result': 'Should find emails from john.smith, alice.johnson (2), and alice follow-up',\n",
    "        'tests': ['search', 'date_filtering']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q11',\n",
    "        'question': 'Group my emails by sender',\n",
    "        'type': 'grouping',\n",
    "        'expected_result': 'Should group by unique senders (7 groups)',\n",
    "        'tests': ['search', 'data_organization']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q12',\n",
    "        'question': 'What is the project completion status mentioned by john.smith@company.com?',\n",
    "        'type': 'content_extraction',\n",
    "        'expected_result': 'Should find \"60% of Phase 1 completed\"',\n",
    "        'tests': ['threading', 'content_understanding']\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(test_questions)} test questions\\\\n\")\n",
    "print(\"Test questions:\")\n",
    "for q in test_questions:\n",
    "    print(f\"  {q['id']}: {q['question']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fc5356",
   "metadata": {},
   "source": [
    "## Section 4: Initialize Email Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from email_agent import EmailAgent, authenticate_gmail, ElasticsearchEmailStore\n",
    "\n",
    "print(\"üîê Initializing agent...\\\\n\")\n",
    "\n",
    "try:\n",
    "    # Initialize Gmail\n",
    "    gmail_service = authenticate_gmail()\n",
    "    print(\"‚úÖ Gmail authenticated\")\n",
    "    \n",
    "    # Initialize Elasticsearch\n",
    "    es_host = os.getenv(\"ES_HOST\", \"localhost\")\n",
    "    es_port = int(os.getenv(\"ES_PORT\", \"9200\"))\n",
    "    es_store = ElasticsearchEmailStore(host=es_host, port=es_port)\n",
    "    print(f\"‚úÖ Elasticsearch connected ({es_host}:{es_port})\")\n",
    "    \n",
    "    # Create agent\n",
    "    agent = EmailAgent(\n",
    "        gmail_service=gmail_service,\n",
    "        es_store=es_store,\n",
    "        model=\"gpt-4o-mini\"\n",
    "    )\n",
    "    print(\"‚úÖ Agent initialized\\\\n\")\n",
    "    print(\"Agent is ready to answer questions!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Agent initialization failed: {e}\")\n",
    "    print(\"\\\\nMake sure:\")\n",
    "    print(\"  1. Elasticsearch is running (docker run -d --name elasticsearch ...)\")\n",
    "    print(\"  2. Gmail credentials are configured (.env file)\")\n",
    "    print(\"  3. OPENAI_API_KEY is set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3333da",
   "metadata": {},
   "source": [
    "## Section 5: Ask Questions and Evaluate Responses\n",
    "\n",
    "**Key Question: Do the results make sense?**\n",
    "\n",
    "Interact with the agent and record your findings for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8b552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for analysis\n",
    "evaluation_results = []\n",
    "\n",
    "print(\"üöÄ Starting Evaluation\\\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, test_q in enumerate(test_questions, 1):\n",
    "    print(f\"\\\\n[Q{i}/{len(test_questions)}] {test_q['question']}\")\n",
    "    print(f\"Type: {test_q['type']}\")\n",
    "    print(f\"Expected: {test_q['expected_result']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Get agent response\n",
    "        response = agent.chat(test_q['question'])\n",
    "        \n",
    "        # Display response\n",
    "        print(f\"\\\\nü§ñ Agent Response:\")\n",
    "        print(response[:500])  # Show first 500 chars\n",
    "        if len(response) > 500:\n",
    "            print(\"\\\\n[... response truncated ...]\")\n",
    "        \n",
    "        # Record result\n",
    "        evaluation_results.append({\n",
    "            'q_id': test_q['id'],\n",
    "            'question': test_q['question'],\n",
    "            'expected': test_q['expected_result'],\n",
    "            'response': response,\n",
    "            'status': 'completed'\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\\\n‚ùå Error: {str(e)}\")\n",
    "        evaluation_results.append({\n",
    "            'q_id': test_q['id'],\n",
    "            'question': test_q['question'],\n",
    "            'expected': test_q['expected_result'],\n",
    "            'response': f\"ERROR: {str(e)}\",\n",
    "            'status': 'error'\n",
    "        })\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(f\"\\\\n‚úÖ Evaluation complete! Tested {len(test_questions)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75e41b",
   "metadata": {},
   "source": [
    "## Section 6: Document Your Findings\n",
    "\n",
    "Record which questions worked, which didn't, and what needs to be noted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e67c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation dataframe\n",
    "df_eval = pd.DataFrame(evaluation_results)\n",
    "\n",
    "print(\"\\\\nüìä EVALUATION SUMMARY\\\\n\")\n",
    "print(f\"Total Questions: {len(df_eval)}\")\n",
    "print(f\"Completed: {len(df_eval[df_eval['status'] == 'completed'])}\")\n",
    "print(f\"Errors: {len(df_eval[df_eval['status'] == 'error'])}\")\n",
    "\n",
    "# Show results table\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"QUESTIONS ASKED:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, row in df_eval.iterrows():\n",
    "    q_num = idx + 1\n",
    "    status_icon = \"‚úÖ\" if row['status'] == 'completed' else \"‚ùå\"\n",
    "    print(f\"\\\\n{status_icon} [{q_num}] {row['question']}\")\n",
    "    print(f\"   Expected: {row['expected']}\")\n",
    "    if row['status'] == 'completed':\n",
    "        preview = row['response'][:150]\n",
    "        print(f\"   Got: {preview}...\" if len(row['response']) > 150 else f\"   Got: {row['response']}\")\n",
    "    else:\n",
    "        print(f\"   Error: {row['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2431192",
   "metadata": {},
   "source": [
    "## Section 7: Manual Evaluation - Do the Results Make Sense?\n",
    "\n",
    "For each question, manually assess if the agent's response is correct and makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc771ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual evaluation summary - fill this in after reviewing all responses\n",
    "\n",
    "print(\"\\\\nüìù EVALUATION FINDINGS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\\\nReview each question's response and answer:\")\n",
    "print(\"  1. Does the response make sense?\")\n",
    "print(\"  2. Does it match the expected result?\")\n",
    "print(\"  3. Any issues or missing features?\\\\n\")\n",
    "\n",
    "print(\"Example findings template:\")\n",
    "print(\"\"\"\n",
    "q1: Sender search from LuxC Mar\n",
    "    Makes sense? ‚úÖ YES\n",
    "    Matches expected? ‚úÖ YES - Found both invoices\n",
    "    Issues? None\n",
    "\n",
    "q2: Total invoice amount\n",
    "    Makes sense? ‚ö†Ô∏è  PARTIAL - Found invoices but didn't calculate total\n",
    "    Matches expected? ‚ùå NO - Expected sum but got list\n",
    "    Issues: Agent should perform calculations on extracted amounts\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"YOUR FINDINGS (fill in after reviewing):\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2906e8bd",
   "metadata": {},
   "source": [
    "## Section 8: Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89979e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save as CSV\n",
    "csv_file = f\"evaluation_results_{timestamp}.csv\"\n",
    "df_eval.to_csv(csv_file, index=False)\n",
    "print(f\"‚úÖ Saved results to {csv_file}\")\n",
    "\n",
    "# Save as JSON for detailed analysis\n",
    "json_file = f\"evaluation_results_{timestamp}.json\"\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "print(f\"‚úÖ Saved detailed results to {json_file}\")\n",
    "\n",
    "# Save test questions reference\n",
    "questions_file = f\"test_questions_{timestamp}.json\"\n",
    "with open(questions_file, 'w') as f:\n",
    "    json.dump(test_questions, f, indent=2)\n",
    "print(f\"‚úÖ Saved test questions to {questions_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8683017b",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b5ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add email_agent to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9df978",
   "metadata": {},
   "source": [
    "## Section 2: Generate Synthetic Email Data\n",
    "\n",
    "Create realistic test emails based on your use case. These represent the data your agent will work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ef406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define synthetic email dataset for testing\n",
    "# Based on typical business email scenarios: invoices, project updates, HR communications\n",
    "\n",
    "synthetic_emails = [\n",
    "    {\n",
    "        'from': 'accounting@luxcmar.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'Invoice #INV-2025-001 for January Services',\n",
    "        'body': 'Please find attached invoice for services rendered in January. Total amount due: $5,250. Payment terms: Net 30 days.',\n",
    "        'date': (datetime.now() - timedelta(days=5)).isoformat(),\n",
    "        'has_attachments': True,\n",
    "        'attachments': ['invoice_january.pdf']\n",
    "    },\n",
    "    {\n",
    "        'from': 'accounting@luxcmar.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'Invoice #INV-2025-002 for February Services',\n",
    "        'body': 'Please find attached invoice for February services. Total: $7,500. Please remit payment within 30 days.',\n",
    "        'date': (datetime.now() - timedelta(days=3)).isoformat(),\n",
    "        'has_attachments': True,\n",
    "        'attachments': ['invoice_february.pdf']\n",
    "    },\n",
    "    {\n",
    "        'from': 'john.smith@company.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'RE: Project Alpha Status Update',\n",
    "        'body': 'The project is on schedule. We completed 60% of Phase 1. Team will present findings next week.',\n",
    "        'date': (datetime.now() - timedelta(days=2)).isoformat(),\n",
    "        'has_attachments': False,\n",
    "        'attachments': []\n",
    "    },\n",
    "    {\n",
    "        'from': 'john.smith@company.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'Project Alpha - Technical Specifications',\n",
    "        'body': 'Attached are the technical specs for Project Alpha. Please review and provide feedback by Friday.',\n",
    "        'date': (datetime.now() - timedelta(days=1)).isoformat(),\n",
    "        'has_attachments': True,\n",
    "        'attachments': ['project_alpha_specs.pdf', 'requirements.docx']\n",
    "    },\n",
    "    {\n",
    "        'from': 'hr@company.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'Benefits Renewal - Action Required',\n",
    "        'body': 'Your benefits renewal period is open. Please review available plans and enroll by March 31st.',\n",
    "        'date': (datetime.now() - timedelta(days=7)).isoformat(),\n",
    "        'has_attachments': True,\n",
    "        'attachments': ['benefits_guide_2025.pdf']\n",
    "    },\n",
    "    {\n",
    "        'from': 'hr@company.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'RE: Benefits Renewal - Action Required',\n",
    "        'body': 'Hi, just a reminder that enrollment closes on March 31st. Let me know if you have questions.',\n",
    "        'date': (datetime.now() - timedelta(days=5)).isoformat(),\n",
    "        'has_attachments': False,\n",
    "        'attachments': []\n",
    "    },\n",
    "    {\n",
    "        'from': 'sales@vendor.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'Q1 2025 Contract - Review and Sign',\n",
    "        'body': 'Please review and sign the attached Q1 contract. Total value: $25,000 for software licenses.',\n",
    "        'date': (datetime.now() - timedelta(days=10)).isoformat(),\n",
    "        'has_attachments': True,\n",
    "        'attachments': ['contract_q1_2025.pdf']\n",
    "    },\n",
    "    {\n",
    "        'from': 'alice.johnson@project.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'Meeting Notes - Strategic Planning',\n",
    "        'body': 'Please find meeting notes from today\\'s strategic planning session. Action items assigned to team.',\n",
    "        'date': (datetime.now() - timedelta(days=1)).isoformat(),\n",
    "        'has_attachments': True,\n",
    "        'attachments': ['meeting_notes_2025-01-24.docx']\n",
    "    },\n",
    "    {\n",
    "        'from': 'alice.johnson@project.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'RE: Meeting Notes - Strategic Planning',\n",
    "        'body': 'Follow-up: Can you please provide your input on Section 3 by tomorrow?',\n",
    "        'date': (datetime.now() - timedelta(hours=2)).isoformat(),\n",
    "        'has_attachments': False,\n",
    "        'attachments': []\n",
    "    },\n",
    "    {\n",
    "        'from': 'compliance@company.com',\n",
    "        'to': 'me@example.com',\n",
    "        'subject': 'Annual Compliance Training - Complete by March 15',\n",
    "        'body': 'All employees must complete annual compliance training. Click link to access course.',\n",
    "        'date': (datetime.now() - timedelta(days=15)).isoformat(),\n",
    "        'has_attachments': False,\n",
    "        'attachments': []\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Generated {len(synthetic_emails)} synthetic emails\")\n",
    "print(\"\\nSample emails:\")\n",
    "for i, email in enumerate(synthetic_emails[:3], 1):\n",
    "    print(f\"  {i}. From: {email['from']} | Subject: {email['subject']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc8c5d4",
   "metadata": {},
   "source": [
    "## Section 3: Create Test Questions Based on Synthetic Data\n",
    "\n",
    "These questions are designed to test specific agent capabilities with the synthetic emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daa0684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test questions based on the synthetic data\n",
    "# These test different agent capabilities\n",
    "\n",
    "test_questions = [\n",
    "    {\n",
    "        'id': 'q1',\n",
    "        'question': 'Show me all invoices from accounting@luxcmar.com',\n",
    "        'type': 'sender_search',\n",
    "        'expected_result': 'Should find 2 invoices (INV-2025-001 and INV-2025-002)',\n",
    "        'tests': ['conversation_history', 'sender_filtering']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q2',\n",
    "        'question': 'What is the total amount of invoices?',\n",
    "        'type': 'data_extraction',\n",
    "        'expected_result': 'Should identify $5,250 + $7,500 = $12,750 total',\n",
    "        'tests': ['attachment_parsing', 'content_understanding']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q3',\n",
    "        'question': 'What is the deadline for the HR benefits enrollment?',\n",
    "        'type': 'deadline_extraction',\n",
    "        'expected_result': 'Should find March 31st deadline',\n",
    "        'tests': ['search', 'content_understanding']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q4',\n",
    "        'question': 'Show all emails from john.smith@company.com',\n",
    "        'type': 'sender_search',\n",
    "        'expected_result': 'Should find 2 emails about Project Alpha',\n",
    "        'tests': ['conversation_history', 'threading']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q5',\n",
    "        'question': 'What files did john.smith@company.com attach?',\n",
    "        'type': 'attachment_list',\n",
    "        'expected_result': 'Should list project_alpha_specs.pdf and requirements.docx',\n",
    "        'tests': ['attachment_handling', 'threading']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q6',\n",
    "        'question': 'Find all emails with attachments',\n",
    "        'type': 'filter_by_attachment',\n",
    "        'expected_result': 'Should find 6 emails with attachments',\n",
    "        'tests': ['search', 'attachment_filtering']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q7',\n",
    "        'question': 'What is the contract value with the vendor?',\n",
    "        'type': 'data_extraction',\n",
    "        'expected_result': 'Should find $25,000 contract value',\n",
    "        'tests': ['attachment_parsing', 'search']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q8',\n",
    "        'question': 'Show me the conversation thread with alice.johnson@project.com',\n",
    "        'type': 'thread_conversation',\n",
    "        'expected_result': 'Should show both emails in chronological order (original + follow-up)',\n",
    "        'tests': ['threading', 'conversation_history']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q9',\n",
    "        'question': 'What are the action items from the strategic planning meeting?',\n",
    "        'type': 'action_extraction',\n",
    "        'expected_result': 'Should parse meeting notes and identify action items',\n",
    "        'tests': ['attachment_parsing', 'content_understanding']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q10',\n",
    "        'question': 'Which emails arrived in the last 3 days?',\n",
    "        'type': 'date_filter',\n",
    "        'expected_result': 'Should find emails from john.smith, alice.johnson (2), and alice follow-up',\n",
    "        'tests': ['search', 'date_filtering']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q11',\n",
    "        'question': 'Group my emails by sender',\n",
    "        'type': 'grouping',\n",
    "        'expected_result': 'Should group by unique senders (7 groups)',\n",
    "        'tests': ['search', 'data_organization']\n",
    "    },\n",
    "    {\n",
    "        'id': 'q12',\n",
    "        'question': 'What is the project completion status mentioned by john.smith@company.com?',\n",
    "        'type': 'content_extraction',\n",
    "        'expected_result': 'Should find \"60% of Phase 1 completed\"',\n",
    "        'tests': ['threading', 'content_understanding']\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(test_questions)} test questions\\n\")\n",
    "print(\"Test questions:\")\n",
    "for q in test_questions:\n",
    "    print(f\"  {q['id']}: {q['question']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff9145",
   "metadata": {},
   "source": [
    "## Section 4: Initialize Email Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b33890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from email_agent import EmailAgent, authenticate_gmail, ElasticsearchEmailStore\n",
    "\n",
    "print(\"üîê Initializing agent...\\n\")\n",
    "\n",
    "try:\n",
    "    # Initialize Gmail\n",
    "    gmail_service = authenticate_gmail()\n",
    "    print(\"‚úÖ Gmail authenticated\")\n",
    "    \n",
    "    # Initialize Elasticsearch\n",
    "    es_host = os.getenv(\"ES_HOST\", \"localhost\")\n",
    "    es_port = int(os.getenv(\"ES_PORT\", \"9200\"))\n",
    "    es_store = ElasticsearchEmailStore(host=es_host, port=es_port)\n",
    "    print(f\"‚úÖ Elasticsearch connected ({es_host}:{es_port})\")\n",
    "    \n",
    "    # Create agent\n",
    "    agent = EmailAgent(\n",
    "        gmail_service=gmail_service,\n",
    "        es_store=es_store,\n",
    "        model=\"gpt-4o-mini\"\n",
    "    )\n",
    "    print(\"‚úÖ Agent initialized\\n\")\n",
    "    print(\"Agent is ready to answer questions!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Agent initialization failed: {e}\")\n",
    "    print(\"\\nMake sure:\")\n",
    "    print(\"  1. Elasticsearch is running (docker run -d --name elasticsearch ...)\")\n",
    "    print(\"  2. Gmail credentials are configured (.env file)\")\n",
    "    print(\"  3. OPENAI_API_KEY is set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8f62c",
   "metadata": {},
   "source": [
    "## Section 5: Ask Questions and Evaluate Responses\n",
    "\n",
    "**Key Question: Do the results make sense?**\n",
    "\n",
    "Interact with the agent and record your findings for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d7d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for analysis\n",
    "evaluation_results = []\n",
    "\n",
    "print(\"üöÄ Starting Evaluation\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, test_q in enumerate(test_questions, 1):\n",
    "    print(f\"\\n[Q{i}/{len(test_questions)}] {test_q['question']}\")\n",
    "    print(f\"Type: {test_q['type']}\")\n",
    "    print(f\"Expected: {test_q['expected_result']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Get agent response\n",
    "        response = agent.chat(test_q['question'])\n",
    "        \n",
    "        # Display response\n",
    "        print(f\"\\nü§ñ Agent Response:\")\n",
    "        print(response[:500])  # Show first 500 chars\n",
    "        if len(response) > 500:\n",
    "            print(\"\\n[... response truncated ...]\")\n",
    "        \n",
    "        # Record result\n",
    "        evaluation_results.append({\n",
    "            'q_id': test_q['id'],\n",
    "            'question': test_q['question'],\n",
    "            'expected': test_q['expected_result'],\n",
    "            'response': response,\n",
    "            'status': 'completed'\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "        evaluation_results.append({\n",
    "            'q_id': test_q['id'],\n",
    "            'question': test_q['question'],\n",
    "            'expected': test_q['expected_result'],\n",
    "            'response': f\"ERROR: {str(e)}\",\n",
    "            'status': 'error'\n",
    "        })\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"\\n‚úÖ Evaluation complete! Tested {len(test_questions)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa65287",
   "metadata": {},
   "source": [
    "## Section 6: Document Your Findings\n",
    "\n",
    "Record which questions worked, which didn't, and what needs to be noted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f05d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation dataframe\n",
    "df_eval = pd.DataFrame(evaluation_results)\n",
    "\n",
    "print(\"\\nüìä EVALUATION SUMMARY\\n\")\n",
    "print(f\"Total Questions: {len(df_eval)}\")\n",
    "print(f\"Completed: {len(df_eval[df_eval['status'] == 'completed'])}\")\n",
    "print(f\"Errors: {len(df_eval[df_eval['status'] == 'error'])}\")\n",
    "\n",
    "# Show results table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUESTIONS ASKED:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, row in df_eval.iterrows():\n",
    "    q_num = idx + 1\n",
    "    status_icon = \"‚úÖ\" if row['status'] == 'completed' else \"‚ùå\"\n",
    "    print(f\"\\n{status_icon} [{q_num}] {row['question']}\")\n",
    "    print(f\"   Expected: {row['expected']}\")\n",
    "    if row['status'] == 'completed':\n",
    "        preview = row['response'][:150]\n",
    "        print(f\"   Got: {preview}...\" if len(row['response']) > 150 else f\"   Got: {row['response']}\")\n",
    "    else:\n",
    "        print(f\"   Error: {row['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0565b59",
   "metadata": {},
   "source": [
    "## Section 7: Manual Evaluation - Do the Results Make Sense?\n",
    "\n",
    "For each question, manually assess if the agent's response is correct and makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f9bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual evaluation summary - fill this in after reviewing all responses\n",
    "\n",
    "print(\"\\nüìù EVALUATION FINDINGS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nReview each question's response and answer:\")\n",
    "print(\"  1. Does the response make sense?\")\n",
    "print(\"  2. Does it match the expected result?\")\n",
    "print(\"  3. Any issues or missing features?\\n\")\n",
    "\n",
    "print(\"Example findings template:\")\n",
    "print(\"\"\"\n",
    "q1: Sender search from LuxC Mar\n",
    "    Makes sense? ‚úÖ YES\n",
    "    Matches expected? ‚úÖ YES - Found both invoices\n",
    "    Issues? None\n",
    "\n",
    "q2: Total invoice amount\n",
    "    Makes sense? ‚ö†Ô∏è  PARTIAL - Found invoices but didn't calculate total\n",
    "    Matches expected? ‚ùå NO - Expected sum but got list\n",
    "    Issues: Agent should perform calculations on extracted amounts\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"YOUR FINDINGS (fill in after reviewing):\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816b636a",
   "metadata": {},
   "source": [
    "## Section 8: Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b658d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save as CSV\n",
    "csv_file = f\"evaluation_results_{timestamp}.csv\"\n",
    "df_eval.to_csv(csv_file, index=False)\n",
    "print(f\"‚úÖ Saved results to {csv_file}\")\n",
    "\n",
    "# Save as JSON for detailed analysis\n",
    "json_file = f\"evaluation_results_{timestamp}.json\"\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "print(f\"‚úÖ Saved detailed results to {json_file}\")\n",
    "\n",
    "# Save test questions reference\n",
    "questions_file = f\"test_questions_{timestamp}.json\"\n",
    "with open(questions_file, 'w') as f:\n",
    "    json.dump(test_questions, f, indent=2)\n",
    "print(f\"‚úÖ Saved test questions to {questions_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1933f27",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250e90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add email_agent to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "print(\" Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799966b5",
   "metadata": {},
   "source": [
    "## Section 2: Generate Questions from Documentation\n",
    "\n",
    "First, generate test questions using the LLM from your agent documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b82b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from email_agent.generate_evaluation_data import main as generate_questions, Config\n",
    "\n",
    "# Generate questions from README.md\n",
    "doc_paths = [\"README.md\"]\n",
    "\n",
    "config = Config(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_workers=2,\n",
    "    output_file=\"evaluation_dataset.csv\"\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Generating test questions from documentation...\")\n",
    "print(\"   This uses the LLM to create realistic user queries\\n\")\n",
    "\n",
    "generate_questions(doc_paths, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e2b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and review generated questions\n",
    "if Path(\"evaluation_dataset.csv\").exists():\n",
    "    df_questions = pd.read_csv(\"evaluation_dataset.csv\")\n",
    "    print(f\"\\n‚úÖ Generated {len(df_questions)} questions\\n\")\n",
    "    print(\"Sample questions:\")\n",
    "    print(df_questions[['question', 'difficulty', 'intent']].head(10).to_string())\n",
    "    print(f\"\\nDistribution:\")\n",
    "    print(df_questions['difficulty'].value_counts())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  CSV file not created yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d02f506",
   "metadata": {},
   "source": [
    "## Section 3: Initialize Email Agent\n",
    "\n",
    "Set up the agent with Gmail and Elasticsearch for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d3e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from email_agent import EmailAgent, authenticate_gmail, ElasticsearchEmailStore\n",
    "\n",
    "print(\"üîê Initializing agent...\")\n",
    "\n",
    "# Initialize Gmail\n",
    "gmail_service = authenticate_gmail()\n",
    "\n",
    "# Initialize Elasticsearch\n",
    "es_host = os.getenv(\"ES_HOST\", \"localhost\")\n",
    "es_port = int(os.getenv(\"ES_PORT\", \"9200\"))\n",
    "es_store = ElasticsearchEmailStore(host=es_host, port=es_port)\n",
    "\n",
    "# Create agent\n",
    "agent = EmailAgent(\n",
    "    gmail_service=gmail_service,\n",
    "    es_store=es_store,\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Agent initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff108cb8",
   "metadata": {},
   "source": [
    "## Section 4: Run Evaluation\n",
    "\n",
    "Execute the agent on all generated questions and evaluate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa7a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from email_agent import ManualEvaluator\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = ManualEvaluator(output_dir=\"./evaluations\")\n",
    "\n",
    "# Load dataset from generated CSV\n",
    "dataset = evaluator.load_dataset_from_csv(\"evaluation_dataset.csv\")\n",
    "\n",
    "print(f\"üìä Dataset loaded: {len(dataset.questions)} questions\\n\")\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluator.run_evaluation(agent, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd424d0f",
   "metadata": {},
   "source": [
    "## Section 5: Review Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5953ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display report\n",
    "evaluator.print_report()\n",
    "\n",
    "# Save results\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "evaluator.save_results(f\"evaluations/results_{timestamp}.pkl\")\n",
    "evaluator.save_report(f\"evaluations/report_{timestamp}.txt\")\n",
    "evaluator.save_results_csv(f\"evaluations/results_{timestamp}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5ab2d2",
   "metadata": {},
   "source": [
    "## Section 6: Analyze Results\n",
    "\n",
    "Detailed analysis of evaluation metrics and per-question breakdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcabdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-question performance\n",
    "if results:\n",
    "    data = []\n",
    "    for result in results:\n",
    "        row = {\n",
    "            'question': result.question[:50] + \"...\",\n",
    "            'overall_score': result.overall_score,\n",
    "            'checks_passed': sum(1 for c in result.checks if c.passed),\n",
    "            'total_checks': len(result.checks),\n",
    "            'avg_score': sum(c.score for c in result.checks) / len(result.checks) if result.checks else 0\n",
    "        }\n",
    "        data.append(row)\n",
    "    \n",
    "    df_results = pd.DataFrame(data)\n",
    "    print(\"\\nüìã Per-Question Analysis:\\n\")\n",
    "    print(df_results.to_string())\n",
    "    \n",
    "    print(f\"\\n\\nüìä Summary Statistics:\")\n",
    "    print(f\"  Best score: {df_results['overall_score'].max():.1f}%\")\n",
    "    print(f\"  Worst score: {df_results['overall_score'].min():.1f}%\")\n",
    "    print(f\"  Average: {df_results['overall_score'].mean():.1f}%\")\n",
    "    print(f\"  Median: {df_results['overall_score'].median():.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd659bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe for analysis\n",
    "results_data = []\n",
    "for result in results:\n",
    "    row = {\n",
    "        'question': result.question,\n",
    "        'overall_score': result.overall_score,\n",
    "    }\n",
    "    for check in result.checks:\n",
    "        row[check.name.value] = check.score\n",
    "    results_data.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"üìä Results Summary Statistics:\")\n",
    "print(f\"  Average Score: {df_results['overall_score'].mean():.1f}%\")\n",
    "print(f\"  Median Score: {df_results['overall_score'].median():.1f}%\")\n",
    "print(f\"  Min Score: {df_results['overall_score'].min():.1f}%\")\n",
    "print(f\"  Max Score: {df_results['overall_score'].max():.1f}%\")\n",
    "\n",
    "print(\"\\n‚úÖ Per-Check Average Scores:\")\n",
    "check_columns = [col for col in df_results.columns if col != 'question' and col != 'overall_score']\n",
    "for col in check_columns:\n",
    "    print(f\"  {col:25s}: {df_results[col].mean():.1f}/10\")\n",
    "\n",
    "# Display detailed results\n",
    "print(\"\\nüìù Detailed Results:\")\n",
    "print(df_results.to_string())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
