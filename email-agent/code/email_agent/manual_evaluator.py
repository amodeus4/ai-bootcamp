"""Manual evaluation system for email agent."""

import pickle
import json
from datetime import datetime
from pathlib import Path
from typing import List, Optional

import pandas as pd

from .evaluation_schemas import (
    TestQuestion,
    EvaluationDataset,
    EvaluationResult,
    EvaluationCheck,
    CheckName,
)


class ManualEvaluator:
    """Manual evaluation engine for agent responses."""

    def __init__(self, output_dir: str = "./evaluations"):
        """Initialize evaluator.

        Args:
            output_dir: Directory to save evaluation results
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.results: List[EvaluationResult] = []

    def load_dataset_from_csv(self, csv_path: str) -> EvaluationDataset:
        """Load evaluation dataset from CSV generated by generate_evaluation_data.py.

        Args:
            csv_path: Path to CSV file

        Returns:
            EvaluationDataset with loaded questions
        """
        df = pd.read_csv(csv_path)

        questions = []
        for _, row in df.iterrows():
            q = TestQuestion(
                question=row["question"],
                summary_answer=row["summary_answer"],
                difficulty=row["difficulty"],
                intent=row["intent"],
                relevant_docs=row["relevant_docs"],
                filename=row.get("filename", "agent_readme"),
            )
            questions.append(q)

        return EvaluationDataset(
            description=f"Evaluation dataset with {len(questions)} questions",
            questions=questions,
        )

    def run_evaluation(
        self, agent, dataset: EvaluationDataset
    ) -> List[EvaluationResult]:
        """Run evaluation on agent with test dataset.

        Args:
            agent: EmailAgent instance
            dataset: EvaluationDataset with test questions

        Returns:
            List of evaluation results
        """
        self.results = []

        print(f"\nüöÄ Running evaluation with {len(dataset.questions)} questions\n")

        for i, test_q in enumerate(dataset.questions, 1):
            print(f"[{i}/{len(dataset.questions)}] {test_q.question}")

            try:
                # Get agent response
                response = agent.chat(test_q.question)

                # Evaluate response
                result = self._evaluate_response(test_q, response)

                self.results.append(result)
                print(f"  ‚úÖ Score: {result.overall_score:.1f}%\n")

            except Exception as e:
                print(f"  ‚ùå Error: {e}\n")
                result = EvaluationResult(
                    question_id=f"q{i}",
                    question=test_q.question,
                    agent_response=f"ERROR: {str(e)}",
                    checks=[],
                    overall_score=0.0,
                    notes="Execution error",
                )
                self.results.append(result)

        return self.results

    def _evaluate_response(
        self, test_question: TestQuestion, agent_response: str
    ) -> EvaluationResult:
        """Evaluate a single agent response with 8-point rubric.

        Args:
            test_question: Original test question
            agent_response: Agent's response

        Returns:
            EvaluationResult with scores and reasoning
        """
        checks: List[EvaluationCheck] = []

        # Check 1: Answer Relevance
        relevance = self._check_relevance(test_question, agent_response)
        checks.append(relevance)

        # Check 2: Answer Completeness
        completeness = self._check_completeness(test_question, agent_response)
        checks.append(completeness)

        # Check 3: Answer Accuracy
        accuracy = self._check_accuracy(test_question, agent_response)
        checks.append(accuracy)

        # Check 4: Tool Usage Correctness
        tool_correct = self._check_tool_usage_correct(test_question, agent_response)
        checks.append(tool_correct)

        # Check 5: Tool Usage Efficiency
        tool_efficient = self._check_tool_usage_efficient(test_question, agent_response)
        checks.append(tool_efficient)

        # Check 6: Attachment Handling
        attachment = self._check_attachment_handling(test_question, agent_response)
        checks.append(attachment)

        # Check 7: Threading Context
        threading = self._check_threading_context(test_question, agent_response)
        checks.append(threading)

        # Check 8: Clarity
        clarity = self._check_clarity(test_question, agent_response)
        checks.append(clarity)

        # Calculate overall score
        passed_checks = sum(1 for c in checks if c.passed)
        avg_score = sum(c.score for c in checks) / len(checks) if checks else 0
        overall_score = (passed_checks / len(checks) * 100) if checks else 0

        return EvaluationResult(
            question_id=f"q_{datetime.now().timestamp()}",
            question=test_question.question,
            agent_response=agent_response,
            checks=checks,
            overall_score=overall_score,
            notes=f"{passed_checks}/{len(checks)} checks passed, avg score {avg_score:.1f}/10",
        )

    def _check_relevance(self, test_q: TestQuestion, response: str) -> EvaluationCheck:
        """Check if response is relevant to the question."""
        # Simple heuristics
        response_lower = response.lower()
        question_lower = test_q.question.lower()

        keywords = question_lower.split()
        keyword_matches = sum(
            1 for kw in keywords if len(kw) > 3 and kw in response_lower
        )

        relevance_score = min(10, (keyword_matches / max(1, len(keywords))) * 10)

        return EvaluationCheck(
            name=CheckName.ANSWER_RELEVANT,
            passed=relevance_score >= 6,
            score=int(relevance_score),
            reasoning=f"Response contains {keyword_matches} of {len(keywords)} key terms",
        )

    def _check_completeness(
        self, test_q: TestQuestion, response: str
    ) -> EvaluationCheck:
        """Check if response fully addresses the question."""
        # Check response length and content
        words = len(response.split())
        expected_words = 50  # Reasonable minimum

        has_specifics = any(
            term in response.lower()
            for term in ["email", "found", "result", "message", "attachment"]
        )

        completeness_score = min(10, (words / expected_words) * 10)
        if has_specifics:
            completeness_score = min(10, completeness_score + 2)

        return EvaluationCheck(
            name=CheckName.ANSWER_COMPLETE,
            passed=completeness_score >= 6,
            score=int(completeness_score),
            reasoning=f"Response length {words} words, {'includes' if has_specifics else 'lacks'} specific details",
        )

    def _check_accuracy(self, test_q: TestQuestion, response: str) -> EvaluationCheck:
        """Check if response is factually accurate."""
        # Check for error indicators
        errors = response.lower().count("error") + response.lower().count("failed")
        has_data = any(
            term in response.lower()
            for term in ["email", "message", "found", "showing"]
        )

        accuracy_score = 10 if (errors == 0 and has_data) else (5 if errors <= 1 else 2)

        return EvaluationCheck(
            name=CheckName.ANSWER_ACCURATE,
            passed=accuracy_score >= 6,
            score=accuracy_score,
            reasoning=f"{'No errors detected' if errors == 0 else f'{errors} error indicator(s)'}, data present: {has_data}",
        )

    def _check_tool_usage_correct(
        self, test_q: TestQuestion, response: str
    ) -> EvaluationCheck:
        """Check if correct tools were used."""
        response_lower = response.lower()

        # Expected tool patterns based on question intent
        if "attachment" in test_q.relevant_docs.lower():
            expected_pattern = "attachment"
        elif (
            "conversation" in test_q.relevant_docs.lower()
            or "conversation" in test_q.question.lower()
        ):
            expected_pattern = "conversation"
        elif (
            "thread" in test_q.relevant_docs.lower()
            or "thread" in test_q.question.lower()
        ):
            expected_pattern = "thread"
        else:
            expected_pattern = "search"

        has_tool = expected_pattern in response_lower or "email" in response_lower

        tool_score = 10 if has_tool else 3

        return EvaluationCheck(
            name=CheckName.TOOL_USAGE_CORRECT,
            passed=tool_score >= 6,
            score=tool_score,
            reasoning=f"Expected '{expected_pattern}' tool usage, {'found' if has_tool else 'not found'}",
        )

    def _check_tool_usage_efficient(
        self, test_q: TestQuestion, response: str
    ) -> EvaluationCheck:
        """Check if tools were used efficiently."""
        # Check if response got results without excessive explanation
        response_lines = response.strip().split("\n")
        has_result = len(response_lines) >= 2  # At least some structure

        efficiency_score = 8 if has_result else 4

        return EvaluationCheck(
            name=CheckName.TOOL_USAGE_EFFICIENT,
            passed=efficiency_score >= 6,
            score=efficiency_score,
            reasoning=f"Response {'has' if has_result else 'lacks'} structured results",
        )

    def _check_attachment_handling(
        self, test_q: TestQuestion, response: str
    ) -> EvaluationCheck:
        """Check attachment handling if relevant."""
        if (
            "attachment" not in test_q.question.lower()
            and "pdf" not in test_q.question.lower()
        ):
            # Not applicable
            return EvaluationCheck(
                name=CheckName.ATTACHMENT_HANDLING,
                passed=True,
                score=10,
                reasoning="Not applicable to this question",
            )

        response_lower = response.lower()
        has_attachment_mention = (
            "attachment" in response_lower
            or "pdf" in response_lower
            or "document" in response_lower
        )

        attachment_score = 10 if has_attachment_mention else 3

        return EvaluationCheck(
            name=CheckName.ATTACHMENT_HANDLING,
            passed=attachment_score >= 6,
            score=attachment_score,
            reasoning=f"Question requires attachment handling, {'handled' if has_attachment_mention else 'not handled'}",
        )

    def _check_threading_context(
        self, test_q: TestQuestion, response: str
    ) -> EvaluationCheck:
        """Check if email threading context is preserved."""
        if (
            "thread" not in test_q.question.lower()
            and "conversation" not in test_q.question.lower()
        ):
            # Not applicable
            return EvaluationCheck(
                name=CheckName.THREADING_CONTEXT,
                passed=True,
                score=10,
                reasoning="Not applicable to this question",
            )

        response_lower = response.lower()
        has_thread_mention = (
            "thread" in response_lower
            or "conversation" in response_lower
            or "message" in response_lower
        )

        thread_score = 10 if has_thread_mention else 3

        return EvaluationCheck(
            name=CheckName.THREADING_CONTEXT,
            passed=thread_score >= 6,
            score=thread_score,
            reasoning=f"Question requires threading context, {'provided' if has_thread_mention else 'not provided'}",
        )

    def _check_clarity(self, test_q: TestQuestion, response: str) -> EvaluationCheck:
        """Check if response is clear and well-formatted."""
        # Simple clarity heuristics
        has_formatting = "\n" in response or ":" in response
        avg_line_length = sum(len(line) for line in response.split("\n")) / max(
            1, len(response.split("\n"))
        )

        clarity_score = 10 if (has_formatting and avg_line_length < 120) else 6

        return EvaluationCheck(
            name=CheckName.CLARITY,
            passed=clarity_score >= 6,
            score=clarity_score,
            reasoning=f"Response {'is well-formatted' if has_formatting else 'is a single block'}, avg line {avg_line_length:.0f} chars",
        )

    def generate_report(self) -> str:
        """Generate a formatted evaluation report.

        Returns:
            Formatted report string
        """
        if not self.results:
            return "‚ùå No evaluation results to report"

        report_lines = [
            "\n" + "=" * 80,
            "üìä EVALUATION REPORT",
            "=" * 80,
        ]

        # Overall stats
        overall_scores = [r.overall_score for r in self.results]
        avg_score = sum(overall_scores) / len(overall_scores) if overall_scores else 0
        passed_count = sum(1 for s in overall_scores if s >= 60)

        report_lines.extend(
            [
                f"\nüìà OVERALL RESULTS:",
                f"  Questions evaluated: {len(self.results)}",
                f"  Average score: {avg_score:.1f}%",
                f"  Passed (‚â•60%): {passed_count}/{len(self.results)}",
            ]
        )

        # Per-check breakdown
        all_checks = [c for r in self.results for c in r.checks]
        if all_checks:
            report_lines.append(f"\n‚úÖ CHECK BREAKDOWN:")
            check_stats = {}
            for check in all_checks:
                if check.name not in check_stats:
                    check_stats[check.name] = {"passed": 0, "total": 0, "scores": []}
                check_stats[check.name]["total"] += 1
                check_stats[check.name]["scores"].append(check.score)
                if check.passed:
                    check_stats[check.name]["passed"] += 1

            for check_name, stats in check_stats.items():
                pct = (
                    (stats["passed"] / stats["total"] * 100)
                    if stats["total"] > 0
                    else 0
                )
                avg_check_score = (
                    sum(stats["scores"]) / len(stats["scores"])
                    if stats["scores"]
                    else 0
                )
                report_lines.append(
                    f"  {check_name.value:25s}: {stats['passed']:2d}/{stats['total']:2d} ({pct:5.1f}%) avg score {avg_check_score:5.1f}/10"
                )

        # Per-question breakdown
        report_lines.append(f"\nüìù QUESTION BREAKDOWN:")
        for i, result in enumerate(self.results, 1):
            report_lines.append(
                f"\n  Q{i} [{result.overall_score:.0f}%]: {result.question}"
            )
            for check in result.checks:
                status = "‚úÖ" if check.passed else "‚ö†Ô∏è"
                report_lines.append(
                    f"    {status} {check.name.value:25s}: {check.score:2d}/10 - {check.reasoning}"
                )

        report_lines.append("\n" + "=" * 80)

        return "\n".join(report_lines)

    def save_results(self, filepath: str):
        """Save evaluation results to pickle file.

        Args:
            filepath: Path to save pickle file
        """
        with open(filepath, "wb") as f:
            pickle.dump(self.results, f)
        print(f"‚úÖ Saved evaluation results to {filepath}")

    def save_report(self, filepath: str):
        """Save report to text file.

        Args:
            filepath: Path to save report
        """
        report = self.generate_report()
        with open(filepath, "w") as f:
            f.write(report)
        print(f"‚úÖ Saved report to {filepath}")

    def save_results_csv(self, filepath: str):
        """Save results to CSV for analysis.

        Args:
            filepath: Path to save CSV
        """
        data = []
        for result in self.results:
            row = {
                "question": result.question,
                "overall_score": result.overall_score,
            }
            # Add per-check scores
            for check in result.checks:
                row[f"check_{check.name.value}"] = check.score

            data.append(row)

        df = pd.DataFrame(data)
        df.to_csv(filepath, index=False)
        print(f"‚úÖ Saved results CSV to {filepath}")

    def print_report(self):
        """Print formatted report to console."""
        print(self.generate_report())
