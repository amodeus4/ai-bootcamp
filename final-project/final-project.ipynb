{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35819120-5aa0-45b8-b904-d3083afc7559",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59f025f-dfce-4712-b894-3fd0b474662e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scraper...\n",
      "\n",
      "[1/92] Anguilla\n",
      "[2/92] Antigua & Barbuda\n",
      "[3/92] Aruba\n",
      "[4/92] Bahamas\n",
      "[5/92] Barbados\n",
      "[6/92] Bonaire\n",
      "[7/92] British Virgin Islands\n",
      "[8/92] Cayman Islands\n",
      "[9/92] Cuba\n",
      "[10/92] Curacao\n",
      "[11/92] Dominica\n",
      "[12/92] Dominican Republic\n",
      "[13/92] Grenada\n",
      "[14/92] Guadeloupe\n",
      "[15/92] Haiti\n",
      "[16/92] Jamaica\n",
      "[17/92] Martinique\n",
      "[18/92] Montserrat\n",
      "[19/92] Puerto Rico\n",
      "[20/92] Saba\n",
      "[21/92] Sint Maarten\n",
      "[22/92] Spanish Virgin Islands\n",
      "[23/92] St. Barts\n",
      "[24/92] St. Kitts & Nevis\n",
      "[25/92] St. Lucia\n",
      "[26/92] St. Martin\n",
      "[27/92] St. Vincent & the Grenadines\n",
      "[28/92] Statia\n",
      "[29/92] Trinidad & Tobago\n",
      "[30/92] Turks & Caicos\n",
      "[31/92] US Virgin Islands\n",
      "[32/92] Belize\n",
      "[33/92] Canada\n",
      "[34/92] Costa Rica\n",
      "[35/92] El Salvador\n",
      "[36/92] Guatemala\n",
      "[37/92] Honduras\n",
      "[38/92] Mexico\n",
      "[39/92] Nicaragua\n",
      "[40/92] Panama\n",
      "[41/92] St. Pierre and Miquelon\n",
      "Failed St. Pierre and Miquelon - main: 404 Client Error: Not Found for url: https://www.noonsite.com/place/st.-pierre-and-miquelon/\n",
      "Failed St. Pierre and Miquelon - clearance: 404 Client Error: Not Found for url: https://www.noonsite.com/place/st.-pierre-and-miquelon/view/clearance/\n",
      "Failed St. Pierre and Miquelon - security: 404 Client Error: Not Found for url: https://www.noonsite.com/place/st.-pierre-and-miquelon/view/security/\n",
      "[42/92] USA\n",
      "[43/92] Albania\n",
      "[44/92] Algeria\n",
      "[45/92] Bosnia\n",
      "[46/92] Bulgaria\n",
      "[47/92] Croatia\n",
      "[48/92] Cyprus\n",
      "[49/92] Egypt\n",
      "[50/92] France\n",
      "[51/92] Georgia\n",
      "[52/92] Gibraltar\n",
      "[53/92] Greece\n",
      "[54/92] Israel\n",
      "[55/92] Italy\n",
      "[56/92] Lebanon\n",
      "[57/92] Libya\n",
      "[58/92] Malta\n",
      "[59/92] Montenegro\n",
      "[60/92] Morocco\n",
      "[61/92] Principality of Monaco\n",
      "[62/92] Romania\n",
      "[63/92] Russia\n",
      "[64/92] Slovenia\n",
      "[65/92] Spain\n",
      "[66/92] Syria\n",
      "[67/92] Tunisia\n",
      "[68/92] Turkey\n",
      "[69/92] Ukraine\n",
      "[70/92] Bahrain\n",
      "[71/92] Djibouti\n",
      "[72/92] Egypt\n",
      "[73/92] Eritrea\n",
      "[74/92] India\n",
      "[75/92] Jordan\n",
      "[76/92] Kuwait\n",
      "[77/92] Maldives\n",
      "[78/92] Oman\n",
      "[79/92] Qatar\n",
      "[80/92] Saudi Arabia\n",
      "[81/92] Somalia\n",
      "[82/92] Sri Lanka\n",
      "[83/92] Sudan\n",
      "[84/92] United Arab Emirates\n",
      "[85/92] Yemen\n",
      "[86/92] Federated States of Micronesia\n",
      "[87/92] Guam\n",
      "[88/92] Hawaii\n",
      "[89/92] Kiribati\n",
      "[90/92] Marshall Islands\n",
      "[91/92] Northern Marianas\n",
      "[92/92] Palau (Belau)\n",
      "\n",
      "Saved 273 documents to noonsite_documents.json\n",
      "Saved Elasticsearch bulk to elasticsearch_bulk.ndjson\n",
      "\n",
      "Done! 273 documents ready for search/database.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "# Define countries and their sections\n",
    "\n",
    "def generate_country_entry(name: str) -> Dict:\n",
    "    \"\"\"Generate a country entry with standardized URL format.\"\"\"\n",
    "    url_name = name.lower().replace(' ', '-').replace('&', '').replace('(', '').replace(')', '').replace('--', '-')\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"base_url\": f\"https://www.noonsite.com/place/{url_name}/\",\n",
    "        \"sections\": [\"\", \"view/clearance/\", \"view/security/\"]\n",
    "    }\n",
    "\n",
    "# Caribbean\n",
    "caribbean_countries = [\n",
    "    \"Anguilla\", \"Antigua & Barbuda\", \"Aruba\", \"Bahamas\", \"Barbados\", \"Bonaire\", \n",
    "    \"British Virgin Islands\", \"Cayman Islands\", \"Cuba\", \"Curacao\", \"Dominica\", \n",
    "    \"Dominican Republic\", \"Grenada\", \"Guadeloupe\", \"Haiti\", \"Jamaica\", \"Martinique\", \n",
    "    \"Montserrat\", \"Puerto Rico\", \"Saba\", \"Sint Maarten\", \"Spanish Virgin Islands\", \n",
    "    \"St. Barts\", \"St. Kitts & Nevis\", \"St. Lucia\", \"St. Martin\", \n",
    "    \"St. Vincent & the Grenadines\", \"Statia\", \"Trinidad & Tobago\", \"Turks & Caicos\", \n",
    "    \"US Virgin Islands\"\n",
    "]\n",
    "\n",
    "# Central & North America\n",
    "central_north_america = [\n",
    "    \"Belize\", \"Canada\", \"Costa Rica\", \"El Salvador\", \"Guatemala\", \"Honduras\", \n",
    "    \"Mexico\", \"Nicaragua\", \"Panama\", \"St. Pierre and Miquelon\", \"USA\"\n",
    "]\n",
    "\n",
    "# Mediterranean & Black Sea\n",
    "mediterranean_black_sea = [\n",
    "    \"Albania\", \"Algeria\", \"Bosnia\", \"Bulgaria\", \"Croatia\", \"Cyprus\", \"Egypt\", \n",
    "    \"France\", \"Georgia\", \"Gibraltar\", \"Greece\", \"Israel\", \"Italy\", \"Lebanon\", \n",
    "    \"Libya\", \"Malta\", \"Montenegro\", \"Morocco\", \"Principality of Monaco\", \"Romania\", \n",
    "    \"Russia\", \"Slovenia\", \"Spain\", \"Syria\", \"Tunisia\", \"Turkey\", \"Ukraine\"\n",
    "]\n",
    "\n",
    "# North Indian Ocean & Red Sea\n",
    "north_indian_ocean = [\n",
    "    \"Bahrain\", \"Djibouti\", \"Egypt\", \"Eritrea\", \"India\", \"Jordan\", \"Kuwait\", \n",
    "    \"Maldives\", \"Oman\", \"Qatar\", \"Saudi Arabia\", \"Somalia\", \"Sri Lanka\", \"Sudan\", \n",
    "    \"United Arab Emirates\", \"Yemen\"\n",
    "]\n",
    "\n",
    "# North Pacific Islands\n",
    "north_pacific_islands = [\n",
    "    \"Federated States of Micronesia\", \"Guam\", \"Hawaii\", \"Kiribati\", \n",
    "    \"Marshall Islands\", \"Northern Marianas\", \"Palau (Belau)\"\n",
    "]\n",
    "\n",
    "# Combine all countries\n",
    "all_country_names = (\n",
    "    caribbean_countries + \n",
    "    central_north_america + \n",
    "    mediterranean_black_sea + \n",
    "    north_indian_ocean + \n",
    "    north_pacific_islands\n",
    ")\n",
    "\n",
    "COUNTRIES = [generate_country_entry(name) for name in all_country_names]\n",
    "\n",
    "# -----------------------------\n",
    "# Function to scrape a single page\n",
    "\n",
    "def scrape_page(url: str) -> str:\n",
    "    \"\"\"Scrape content from a single page and stop at 'Next Section'.\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Remove all navigation, sidebars, headers, footers, and ads\n",
    "    for element in soup.find_all(['nav', 'header', 'footer', 'aside']):\n",
    "        element.decompose()\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    unwanted_selectors = [\n",
    "        '.navigation', '.menu', '.sidebar', '.widget', '.advertisement',\n",
    "        '#navigation', '#menu', '#sidebar', '.footer', '.header',\n",
    "        '[class*=\"nav\"]', '[id*=\"nav\"]', '[class*=\"menu\"]',\n",
    "        'form', 'button', '.search', '[role=\"navigation\"]',\n",
    "        '[class*=\"discount\"]', '[class*=\"flag\"]', '[class*=\"coupon\"]'\n",
    "    ]\n",
    "    \n",
    "    for selector in unwanted_selectors:\n",
    "        for element in soup.select(selector):\n",
    "            element.decompose()\n",
    "    \n",
    "    # Find main content\n",
    "    main_content = None\n",
    "    content_selectors = [\n",
    "        'article',\n",
    "        'div[class*=\"content\"]',\n",
    "        'div[class*=\"entry\"]',\n",
    "        'main',\n",
    "        '#content'\n",
    "    ]\n",
    "    \n",
    "    for selector in content_selectors:\n",
    "        main_content = soup.select_one(selector)\n",
    "        if main_content and len(main_content.get_text(strip=True)) > 200:\n",
    "            break\n",
    "    \n",
    "    if not main_content:\n",
    "        main_content = soup.body\n",
    "    \n",
    "    # Extract text\n",
    "    text_parts = []\n",
    "    \n",
    "    if main_content:\n",
    "        for element in main_content.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'p', 'li', 'ul', 'ol']):\n",
    "            text = element.get_text(strip=True)\n",
    "            \n",
    "            # Stop if we hit \"Next Section\"\n",
    "            if 'next section' in text.lower():\n",
    "                break\n",
    "            \n",
    "            # Skip navigation-like text\n",
    "            if text and len(text) > 10 and not is_navigation_text(text):\n",
    "                text_parts.append(text)\n",
    "    \n",
    "    content_text = '\\n\\n'.join(text_parts)\n",
    "    return clean_text(content_text)\n",
    "\n",
    "def is_navigation_text(text: str) -> bool:\n",
    "    \"\"\"Check if text looks like navigation/menu items.\"\"\"\n",
    "    navigation_keywords = [\n",
    "        'login', 'register', 'search', 'navigate to', 'select country',\n",
    "        'close', 'menu', 'download', 'subscribe', 'notification',\n",
    "        'cookie', 'privacy policy', 'terms of service', 'buy now',\n",
    "        'discount', 'coupon', 'yachtflags', 'courtesy flag'\n",
    "    ]\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    if len(text) < 30 and text.count('\\n') == 0:\n",
    "        return True\n",
    "    \n",
    "    return any(keyword in text_lower for keyword in navigation_keywords)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean up extracted text.\"\"\"\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if len(line) > 15 or (len(line) > 5 and '.' in line):\n",
    "            if not (len(line) < 40 and line[0].isupper() and line.replace(' ', '').isalpha()):\n",
    "                cleaned_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "# -----------------------------\n",
    "# Scrape all sections for a country\n",
    "\n",
    "def scrape_country(country: Dict) -> List[Dict]:\n",
    "    \"\"\"Scrape all sections for a given country and return as separate documents.\"\"\"\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    for section in country['sections']:\n",
    "        url = country['base_url'] + section\n",
    "        section_name = section.replace('view/', '').replace('/', '') or \"main\"\n",
    "        \n",
    "        try:\n",
    "            content = scrape_page(url)\n",
    "            \n",
    "            if content:\n",
    "                doc = {\n",
    "                    \"id\": f\"{country['name'].lower().replace(' ', '_')}_{section_name}\",\n",
    "                    \"country\": country['name'],\n",
    "                    \"section_type\": section_name,\n",
    "                    \"url\": url,\n",
    "                    \"content\": content\n",
    "                }\n",
    "                documents.append(doc)\n",
    "            \n",
    "            time.sleep(1.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed {country['name']} - {section_name}: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# -----------------------------\n",
    "# Scrape all countries\n",
    "\n",
    "def scrape_all_countries(countries: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Scrape all countries and return list of documents.\"\"\"\n",
    "    all_documents = []\n",
    "    \n",
    "    for i, country in enumerate(countries, 1):\n",
    "        print(f\"[{i}/{len(countries)}] {country['name']}\")\n",
    "        docs = scrape_country(country)\n",
    "        all_documents.extend(docs)\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "# -----------------------------\n",
    "# Save documents\n",
    "\n",
    "def save_documents(documents: List[Dict], filename: str = \"noonsite_documents.json\"):\n",
    "    \"\"\"Save documents as JSON.\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(documents, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nSaved {len(documents)} documents to {filename}\")\n",
    "\n",
    "def save_elasticsearch_bulk(documents: List[Dict], filename: str = \"elasticsearch_bulk.ndjson\"):\n",
    "    \"\"\"Save as Elasticsearch bulk format.\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for doc in documents:\n",
    "            action = {\"index\": {\"_index\": \"noonsite\", \"_id\": doc[\"id\"]}}\n",
    "            f.write(json.dumps(action, ensure_ascii=False) + '\\n')\n",
    "            f.write(json.dumps(doc, ensure_ascii=False) + '\\n')\n",
    "    print(f\"Saved Elasticsearch bulk to {filename}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Run \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting scraper...\\n\")\n",
    "    \n",
    "    documents = scrape_all_countries(COUNTRIES)\n",
    "    \n",
    "    save_documents(documents)\n",
    "    save_elasticsearch_bulk(documents)\n",
    "    \n",
    "    print(f\"\\nDone! {len(documents)} documents ready\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
