{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35819120-5aa0-45b8-b904-d3083afc7559",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59f025f-dfce-4712-b894-3fd0b474662e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scraper...\n",
      "\n",
      "[1/92] Anguilla\n",
      "[2/92] Antigua & Barbuda\n",
      "[3/92] Aruba\n",
      "[4/92] Bahamas\n",
      "[5/92] Barbados\n",
      "[6/92] Bonaire\n",
      "[7/92] British Virgin Islands\n",
      "[8/92] Cayman Islands\n",
      "[9/92] Cuba\n",
      "[10/92] Curacao\n",
      "[11/92] Dominica\n",
      "[12/92] Dominican Republic\n",
      "[13/92] Grenada\n",
      "[14/92] Guadeloupe\n",
      "[15/92] Haiti\n",
      "[16/92] Jamaica\n",
      "[17/92] Martinique\n",
      "[18/92] Montserrat\n",
      "[19/92] Puerto Rico\n",
      "[20/92] Saba\n",
      "[21/92] Sint Maarten\n",
      "[22/92] Spanish Virgin Islands\n",
      "[23/92] St. Barts\n",
      "[24/92] St. Kitts & Nevis\n",
      "[25/92] St. Lucia\n",
      "[26/92] St. Martin\n",
      "[27/92] St. Vincent & the Grenadines\n",
      "[28/92] Statia\n",
      "[29/92] Trinidad & Tobago\n",
      "[30/92] Turks & Caicos\n",
      "[31/92] US Virgin Islands\n",
      "[32/92] Belize\n",
      "[33/92] Canada\n",
      "[34/92] Costa Rica\n",
      "[35/92] El Salvador\n",
      "[36/92] Guatemala\n",
      "[37/92] Honduras\n",
      "[38/92] Mexico\n",
      "[39/92] Nicaragua\n",
      "[40/92] Panama\n",
      "[41/92] St. Pierre and Miquelon\n",
      "Failed St. Pierre and Miquelon - main: 404 Client Error: Not Found for url: https://www.noonsite.com/place/st.-pierre-and-miquelon/\n",
      "Failed St. Pierre and Miquelon - clearance: 404 Client Error: Not Found for url: https://www.noonsite.com/place/st.-pierre-and-miquelon/view/clearance/\n",
      "Failed St. Pierre and Miquelon - security: 404 Client Error: Not Found for url: https://www.noonsite.com/place/st.-pierre-and-miquelon/view/security/\n",
      "[42/92] USA\n",
      "[43/92] Albania\n",
      "[44/92] Algeria\n",
      "[45/92] Bosnia\n",
      "[46/92] Bulgaria\n",
      "[47/92] Croatia\n",
      "[48/92] Cyprus\n",
      "[49/92] Egypt\n",
      "[50/92] France\n",
      "[51/92] Georgia\n",
      "[52/92] Gibraltar\n",
      "[53/92] Greece\n",
      "[54/92] Israel\n",
      "[55/92] Italy\n",
      "[56/92] Lebanon\n",
      "[57/92] Libya\n",
      "[58/92] Malta\n",
      "[59/92] Montenegro\n",
      "[60/92] Morocco\n",
      "[61/92] Principality of Monaco\n",
      "[62/92] Romania\n",
      "[63/92] Russia\n",
      "[64/92] Slovenia\n",
      "[65/92] Spain\n",
      "[66/92] Syria\n",
      "[67/92] Tunisia\n",
      "[68/92] Turkey\n",
      "[69/92] Ukraine\n",
      "[70/92] Bahrain\n",
      "[71/92] Djibouti\n",
      "[72/92] Egypt\n",
      "[73/92] Eritrea\n",
      "[74/92] India\n",
      "[75/92] Jordan\n",
      "[76/92] Kuwait\n",
      "[77/92] Maldives\n",
      "[78/92] Oman\n",
      "[79/92] Qatar\n",
      "[80/92] Saudi Arabia\n",
      "[81/92] Somalia\n",
      "[82/92] Sri Lanka\n",
      "[83/92] Sudan\n",
      "[84/92] United Arab Emirates\n",
      "[85/92] Yemen\n",
      "[86/92] Federated States of Micronesia\n",
      "[87/92] Guam\n",
      "[88/92] Hawaii\n",
      "[89/92] Kiribati\n",
      "[90/92] Marshall Islands\n",
      "[91/92] Northern Marianas\n",
      "[92/92] Palau (Belau)\n",
      "\n",
      "Saved 273 documents to noonsite_documents.json\n",
      "Saved Elasticsearch bulk to elasticsearch_bulk.ndjson\n",
      "\n",
      "Done! 273 documents ready for search/database.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "# Define countries and their sections\n",
    "\n",
    "def generate_country_entry(name: str) -> Dict:\n",
    "    \"\"\"Generate a country entry with standardized URL format.\"\"\"\n",
    "    url_name = name.lower().replace(' ', '-').replace('&', '').replace('(', '').replace(')', '').replace('--', '-')\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"base_url\": f\"https://www.noonsite.com/place/{url_name}/\",\n",
    "        \"sections\": [\"\", \"view/clearance/\", \"view/security/\"]\n",
    "    }\n",
    "\n",
    "# Caribbean\n",
    "caribbean_countries = [\n",
    "    \"Anguilla\", \"Antigua & Barbuda\", \"Aruba\", \"Bahamas\", \"Barbados\", \"Bonaire\", \n",
    "    \"British Virgin Islands\", \"Cayman Islands\", \"Cuba\", \"Curacao\", \"Dominica\", \n",
    "    \"Dominican Republic\", \"Grenada\", \"Guadeloupe\", \"Haiti\", \"Jamaica\", \"Martinique\", \n",
    "    \"Montserrat\", \"Puerto Rico\", \"Saba\", \"Sint Maarten\", \"Spanish Virgin Islands\", \n",
    "    \"St. Barts\", \"St. Kitts & Nevis\", \"St. Lucia\", \"St. Martin\", \n",
    "    \"St. Vincent & the Grenadines\", \"Statia\", \"Trinidad & Tobago\", \"Turks & Caicos\", \n",
    "    \"US Virgin Islands\"\n",
    "]\n",
    "\n",
    "# Central & North America\n",
    "central_north_america = [\n",
    "    \"Belize\", \"Canada\", \"Costa Rica\", \"El Salvador\", \"Guatemala\", \"Honduras\", \n",
    "    \"Mexico\", \"Nicaragua\", \"Panama\", \"St. Pierre and Miquelon\", \"USA\"\n",
    "]\n",
    "\n",
    "# Mediterranean & Black Sea\n",
    "mediterranean_black_sea = [\n",
    "    \"Albania\", \"Algeria\", \"Bosnia\", \"Bulgaria\", \"Croatia\", \"Cyprus\", \"Egypt\", \n",
    "    \"France\", \"Georgia\", \"Gibraltar\", \"Greece\", \"Israel\", \"Italy\", \"Lebanon\", \n",
    "    \"Libya\", \"Malta\", \"Montenegro\", \"Morocco\", \"Principality of Monaco\", \"Romania\", \n",
    "    \"Russia\", \"Slovenia\", \"Spain\", \"Syria\", \"Tunisia\", \"Turkey\", \"Ukraine\"\n",
    "]\n",
    "\n",
    "# North Indian Ocean & Red Sea\n",
    "north_indian_ocean = [\n",
    "    \"Bahrain\", \"Djibouti\", \"Egypt\", \"Eritrea\", \"India\", \"Jordan\", \"Kuwait\", \n",
    "    \"Maldives\", \"Oman\", \"Qatar\", \"Saudi Arabia\", \"Somalia\", \"Sri Lanka\", \"Sudan\", \n",
    "    \"United Arab Emirates\", \"Yemen\"\n",
    "]\n",
    "\n",
    "# North Pacific Islands\n",
    "north_pacific_islands = [\n",
    "    \"Federated States of Micronesia\", \"Guam\", \"Hawaii\", \"Kiribati\", \n",
    "    \"Marshall Islands\", \"Northern Marianas\", \"Palau (Belau)\"\n",
    "]\n",
    "\n",
    "# Combine all countries\n",
    "all_country_names = (\n",
    "    caribbean_countries + \n",
    "    central_north_america + \n",
    "    mediterranean_black_sea + \n",
    "    north_indian_ocean + \n",
    "    north_pacific_islands\n",
    ")\n",
    "\n",
    "COUNTRIES = [generate_country_entry(name) for name in all_country_names]\n",
    "\n",
    "# -----------------------------\n",
    "# Function to scrape a single page\n",
    "\n",
    "def scrape_page(url: str) -> str:\n",
    "    \"\"\"Scrape content from a single page and stop at 'Next Section'.\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Remove all navigation, sidebars, headers, footers, and ads\n",
    "    for element in soup.find_all(['nav', 'header', 'footer', 'aside']):\n",
    "        element.decompose()\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    unwanted_selectors = [\n",
    "        '.navigation', '.menu', '.sidebar', '.widget', '.advertisement',\n",
    "        '#navigation', '#menu', '#sidebar', '.footer', '.header',\n",
    "        '[class*=\"nav\"]', '[id*=\"nav\"]', '[class*=\"menu\"]',\n",
    "        'form', 'button', '.search', '[role=\"navigation\"]',\n",
    "        '[class*=\"discount\"]', '[class*=\"flag\"]', '[class*=\"coupon\"]'\n",
    "    ]\n",
    "    \n",
    "    for selector in unwanted_selectors:\n",
    "        for element in soup.select(selector):\n",
    "            element.decompose()\n",
    "    \n",
    "    # Find main content\n",
    "    main_content = None\n",
    "    content_selectors = [\n",
    "        'article',\n",
    "        'div[class*=\"content\"]',\n",
    "        'div[class*=\"entry\"]',\n",
    "        'main',\n",
    "        '#content'\n",
    "    ]\n",
    "    \n",
    "    for selector in content_selectors:\n",
    "        main_content = soup.select_one(selector)\n",
    "        if main_content and len(main_content.get_text(strip=True)) > 200:\n",
    "            break\n",
    "    \n",
    "    if not main_content:\n",
    "        main_content = soup.body\n",
    "    \n",
    "    # Extract text\n",
    "    text_parts = []\n",
    "    \n",
    "    if main_content:\n",
    "        for element in main_content.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'p', 'li', 'ul', 'ol']):\n",
    "            text = element.get_text(strip=True)\n",
    "            \n",
    "            # Stop if we hit \"Next Section\"\n",
    "            if 'next section' in text.lower():\n",
    "                break\n",
    "            \n",
    "            # Skip navigation-like text\n",
    "            if text and len(text) > 10 and not is_navigation_text(text):\n",
    "                text_parts.append(text)\n",
    "    \n",
    "    content_text = '\\n\\n'.join(text_parts)\n",
    "    return clean_text(content_text)\n",
    "\n",
    "def is_navigation_text(text: str) -> bool:\n",
    "    \"\"\"Check if text looks like navigation/menu items.\"\"\"\n",
    "    navigation_keywords = [\n",
    "        'login', 'register', 'search', 'navigate to', 'select country',\n",
    "        'close', 'menu', 'download', 'subscribe', 'notification',\n",
    "        'cookie', 'privacy policy', 'terms of service', 'buy now',\n",
    "        'discount', 'coupon', 'yachtflags', 'courtesy flag'\n",
    "    ]\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    if len(text) < 30 and text.count('\\n') == 0:\n",
    "        return True\n",
    "    \n",
    "    return any(keyword in text_lower for keyword in navigation_keywords)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean up extracted text.\"\"\"\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if len(line) > 15 or (len(line) > 5 and '.' in line):\n",
    "            if not (len(line) < 40 and line[0].isupper() and line.replace(' ', '').isalpha()):\n",
    "                cleaned_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "# -----------------------------\n",
    "# Scrape all sections for a country\n",
    "\n",
    "def scrape_country(country: Dict) -> List[Dict]:\n",
    "    \"\"\"Scrape all sections for a given country and return as separate documents.\"\"\"\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    for section in country['sections']:\n",
    "        url = country['base_url'] + section\n",
    "        section_name = section.replace('view/', '').replace('/', '') or \"main\"\n",
    "        \n",
    "        try:\n",
    "            content = scrape_page(url)\n",
    "            \n",
    "            if content:\n",
    "                doc = {\n",
    "                    \"id\": f\"{country['name'].lower().replace(' ', '_')}_{section_name}\",\n",
    "                    \"country\": country['name'],\n",
    "                    \"section_type\": section_name,\n",
    "                    \"url\": url,\n",
    "                    \"content\": content\n",
    "                }\n",
    "                documents.append(doc)\n",
    "            \n",
    "            time.sleep(1.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed {country['name']} - {section_name}: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# -----------------------------\n",
    "# Scrape all countries\n",
    "\n",
    "def scrape_all_countries(countries: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Scrape all countries and return list of documents.\"\"\"\n",
    "    all_documents = []\n",
    "    \n",
    "    for i, country in enumerate(countries, 1):\n",
    "        print(f\"[{i}/{len(countries)}] {country['name']}\")\n",
    "        docs = scrape_country(country)\n",
    "        all_documents.extend(docs)\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "# -----------------------------\n",
    "# Save documents\n",
    "\n",
    "def save_documents(documents: List[Dict], filename: str = \"noonsite_documents.json\"):\n",
    "    \"\"\"Save documents as JSON.\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(documents, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nSaved {len(documents)} documents to {filename}\")\n",
    "\n",
    "def save_elasticsearch_bulk(documents: List[Dict], filename: str = \"elasticsearch_bulk.ndjson\"):\n",
    "    \"\"\"Save as Elasticsearch bulk format.\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for doc in documents:\n",
    "            action = {\"index\": {\"_index\": \"noonsite\", \"_id\": doc[\"id\"]}}\n",
    "            f.write(json.dumps(action, ensure_ascii=False) + '\\n')\n",
    "            f.write(json.dumps(doc, ensure_ascii=False) + '\\n')\n",
    "    print(f\"Saved Elasticsearch bulk to {filename}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Run \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting scraper...\\n\")\n",
    "    \n",
    "    documents = scrape_all_countries(COUNTRIES)\n",
    "    \n",
    "    save_documents(documents)\n",
    "    save_elasticsearch_bulk(documents)\n",
    "    \n",
    "    print(f\"\\nDone! {len(documents)} documents ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b309f8-0482-44d0-ab35-a42b6655d565",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d087eacc-8755-4bad-848b-1370c1e9384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk by tokens\n",
    "\n",
    "import json\n",
    "\n",
    "with open('noonsite_documents.json', 'r', encoding='utf-8') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "# Sliding window \n",
    "def sliding_window_tokens(tokens, size, step):\n",
    "    results = []\n",
    "    for i in range(0, len(tokens), step):\n",
    "        chunk_tokens = tokens[i:i+size]\n",
    "        results.append({\n",
    "            'start': i,\n",
    "            'content': ' '.join(chunk_tokens)\n",
    "        })\n",
    "        if i + size >= len(tokens):\n",
    "            break\n",
    "    return results\n",
    "\n",
    "# Chunk documents\n",
    "def chunk_documents(documents, chunk_size=500, overlap=100):\n",
    "    chunked = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        content = doc.get('content', '')\n",
    "        tokens = tokenize(content)\n",
    "        \n",
    "        if not tokens:\n",
    "            continue\n",
    "        \n",
    "        step = chunk_size - overlap\n",
    "        chunks = sliding_window_tokens(tokens, size=chunk_size, step=step)\n",
    "        \n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunked_doc = {\n",
    "                'id': f\"{doc['id']}_chunk_{idx}\",\n",
    "                'country': doc['country'],\n",
    "                'section_type': doc['section_type'],\n",
    "                'url': doc['url'],\n",
    "                'content': chunk['content'],\n",
    "                'chunk_index': idx,\n",
    "                'start_token': chunk['start']\n",
    "            }\n",
    "            chunked.append(chunked_doc)\n",
    "    \n",
    "    return chunked\n",
    "\n",
    "\n",
    "chunked_data = chunk_documents(documents, chunk_size=500, overlap=100)\n",
    "with open('noonsite_chunked.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(chosen_chunked, f, indent=2, ensure_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbc78f03-08b9-43cb-9d31-1bb70cdca18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x7c5301b05880>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing\n",
    "\n",
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"country\", \"section_type\", \"content\"],\n",
    ")\n",
    "index.fit(chunked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9433556d-7bf2-4f46-8860-ecdb619d9c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'antigua_&_barbuda_security_chunk_0',\n",
       " 'country': 'Antigua & Barbuda',\n",
       " 'section_type': 'security',\n",
       " 'url': 'https://www.noonsite.com/place/antigua-barbuda/view/security/',\n",
       " 'content': 'Antigua & Barbuda Security for Yachts Based on reports to Noonsite from cruisers, petty theft is on the rise in the Caribbean in general. Cruisers should take basic safety precautions and use common sense when ashore. Avoid walking alone in isolated areas, especially at night. Don’t carry large amounts of cash around or wear expensive jewelry. The risks of petty theft, pickpocketing and assault increases during regattas and festivals. Do not leave drinks unattended. Antigua has the lowest violent crime rate in the Caribbean and severe crime against tourists is rare. Dinghy thieves operate throughout the Caribbean. Be sure to lift, chain, and lock your dinghy and outboard when not in use, especially at night. When leaving your dinghy ashore, ensure that your dinghy and outboard are securely locked. AirTags and other cell based/Bluetooth tracking devices are being used by cruisers to track their dinghys/outboards. Some thieves are aware of this practice and may detect, locate and disable them. Understand the limitations of using tracking devices. (See thisMay 2024 Jost Van Dyke, BVI CSSN report.) The use of GPS trackers (rather than Bluetooth trackers) is recommended. See thisReport by David Lyman. A new method of corruption appearing in Caribbean islands is for a “finders” fee to be demanded when a stolen dinghy is “found”. (See thisNov 2023 Tyrell Bay, Carriacou CSSN reportand thisApr 2024 Clifton Harbour, SVG CSSN report.) Occasional incidents of dinghy theft occur across Antigua & Barbuda. The Caribbean Safety and Security Net (CSSN)collects and shares reliable reports of crimes against yachts in the Caribbean, helping cruisers make informed decisions. TheCSSN websiteoffers current and past incident reports, stats, regional piracy info, and annual summaries. Here are some oftheways to use their site: View the Interactive Mapto view the latest reports in your cruising area. Review Incident Statisticsto view statistics by type, country and time. Report an Incidentif you’ve experienced or witnessed a boarding, robbery, or other security issue, report it via theCSSN Incident Reporting Form. Review CSSN’sGeneral Security Precautions for Cruisers. Visit theCSSN Antigua & Barbuda pageor visit theCSSN Interactive Mapto keep up to date with the latest incident reports. Also review NoonsiteAntigua & Barbuda security reports. SeeEmergenciesfor more details. If you have information for this section, or feedback on businesses used, please let us know at editor@noonsite.com. We also welcome new information about businesses you have used (see Related Businesses).',\n",
       " 'chunk_index': 0,\n",
       " 'start_token': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results = index.search('How safe is Antigue & Barbuda?')\n",
    "search_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b43dbf5-dc33-478b-acad-f8a9bc7973a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic chunking: 273 chunks\n"
     ]
    }
   ],
   "source": [
    "# Semantic chunking - but i don't need this because its already saved and ingested into minsearch\n",
    "\n",
    "def chunk_by_sections(documents):\n",
    "    \"\"\"Chunk by natural sections (headers, FAQs, etc)\"\"\"\n",
    "    chunked = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        content = doc.get('content', '')\n",
    "        \n",
    "       \n",
    "        sections = [s.strip() for s in content.split('\\n\\n') if s.strip()]\n",
    "        \n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "        chunk_idx = 0\n",
    "        \n",
    "        for section in sections:\n",
    "            section_tokens = len(section.split())\n",
    "            \n",
    "            \n",
    "            if current_tokens + section_tokens > 500 and current_chunk:\n",
    "                chunked_doc = {\n",
    "                    'id': f\"{doc['id']}_chunk_{chunk_idx}\",\n",
    "                    'country': doc['country'],\n",
    "                    'section_type': doc['section_type'],\n",
    "                    'url': doc['url'],\n",
    "                    'content': '\\n\\n'.join(current_chunk),\n",
    "                    'chunk_index': chunk_idx\n",
    "                }\n",
    "                chunked.append(chunked_doc)\n",
    "                \n",
    "                # Start new chunk with overlap (last section)\n",
    "                current_chunk = [current_chunk[-1], section]\n",
    "                current_tokens = len(current_chunk[-2].split()) + section_tokens\n",
    "                chunk_idx += 1\n",
    "            else:\n",
    "                current_chunk.append(section)\n",
    "                current_tokens += section_tokens\n",
    "        \n",
    "        # Save remaining content\n",
    "        if current_chunk:\n",
    "            chunked_doc = {\n",
    "                'id': f\"{doc['id']}_chunk_{chunk_idx}\",\n",
    "                'country': doc['country'],\n",
    "                'section_type': doc['section_type'],\n",
    "                'url': doc['url'],\n",
    "                'content': '\\n\\n'.join(current_chunk),\n",
    "                'chunk_index': chunk_idx\n",
    "            }\n",
    "            chunked.append(chunked_doc)\n",
    "    \n",
    "    return chunked\n",
    "\n",
    "# Compare approaches\n",
    "semantic_chunks = chunk_by_sections(documents)\n",
    "print(f\"Semantic chunking: {len(semantic_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3cf67-cabd-4a8b-86b4-b2dcd02268f1",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c4c7911-f20d-46a4-a732-25162666fc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: antigua_&_barbuda_main_chunk_0\n",
      "Country: Antigua & Barbuda\n",
      "Section: main\n",
      "Content snippet: Antigua & Barbuda Facts for Sailors\n",
      "Antigua and Barbuda is a state in the West Indies between the Caribbean Sea and Atlantic Ocean, consisting of the two main islands of Antigua and Barbuda and a number of smaller islands including the uninhabited Redonda Island. Barbuda is approx. 30NM north of Ant...\n",
      "\n",
      "ID: antigua_&_barbuda_clearance_chunk_0\n",
      "Country: Antigua & Barbuda\n",
      "Section: clearance\n",
      "Content snippet: Antigua & Barbuda Pre-Arrival Procedures for Yachts\n",
      "For a full explanation of Caribbean web clearance services see reportEastern Caribbean Cruising: Clearance Procedures Simplified.\n",
      "SeeBarbuda Clearance in Codringtonfor more information.\n",
      "Antigua & Barbuda Arrival Procedures for Yachts\n",
      "Codrington, Ba...\n",
      "\n",
      "ID: antigua_&_barbuda_security_chunk_0\n",
      "Country: Antigua & Barbuda\n",
      "Section: security\n",
      "Content snippet: Antigua & Barbuda Security for Yachts\n",
      "Based on reports to Noonsite from cruisers, petty theft is on the rise in the Caribbean in general. Cruisers should take basic safety precautions and use common sense when ashore. Avoid walking alone in isolated areas, especially at night. Don’t carry large amou...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "semantic_index = Index(\n",
    "    text_fields=[\"country\", \"section_type\", \"content\"]\n",
    ")\n",
    "\n",
    "semantic_index.fit(semantic_chunks)\n",
    "\n",
    "\n",
    "query = \"How safe is Antigua & Barbuda?\"\n",
    "results = semantic_index.search(query)\n",
    "\n",
    "for r in results[:3]:  \n",
    "    print(f\"ID: {r['id']}\")\n",
    "    print(f\"Country: {r['country']}\")\n",
    "    print(f\"Section: {r['section_type']}\")\n",
    "    print(f\"Content snippet: {r['content'][:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb23407f-7f12-493f-a002-9dc95fd858b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data = chunk_documents(documents)\n",
    "semantic_chunks = chunk_by_sections(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f5648d3-83c0-4a90-b6c6-2c755662a78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420 token-based chunks\n",
      "273 semantic chunks\n"
     ]
    }
   ],
   "source": [
    "print(len(chunked_data), \"token-based chunks\")\n",
    "print(len(semantic_chunks), \"semantic chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1942b4f-b604-440a-9392-d48ac900dc00",
   "metadata": {},
   "source": [
    "I'm going to use Elasticsearch for persistant storage. And the data is also already ready to be processed by elasticsearch. Don't really need some of the above code. \n",
    "\n",
    "Add that code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
